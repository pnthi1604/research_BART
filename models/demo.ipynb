{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 4.2927e-01, -5.6774e-01, -3.8427e-01,  3.3193e-01, -1.3516e-01,\n",
      "          -6.2389e-01, -2.4914e-01,  3.7619e-01,  5.7470e-01,  5.8342e-01,\n",
      "           6.3682e-02,  7.2657e-01,  6.4011e-01, -1.1232e-01, -1.7830e-01,\n",
      "           1.0744e-01],\n",
      "         [ 4.3919e-01, -5.2775e-01, -4.9849e-01,  3.2339e-01, -1.3742e-01,\n",
      "          -6.5530e-01, -1.5063e-01,  3.9463e-01,  4.6116e-01,  6.1640e-01,\n",
      "          -5.3225e-02,  8.0006e-01,  6.7477e-01, -8.8532e-02, -6.4851e-02,\n",
      "           5.6645e-02],\n",
      "         [ 4.6637e-01, -4.4480e-01, -5.5922e-01,  3.0401e-01, -1.3780e-01,\n",
      "          -6.5968e-01, -1.4163e-01,  3.7383e-01,  5.4247e-01,  5.8901e-01,\n",
      "          -1.0480e-01,  6.9249e-01,  6.2579e-01, -1.3175e-01,  4.1674e-04,\n",
      "           8.5077e-02],\n",
      "         [ 5.0909e-01, -5.6102e-01, -3.8439e-01,  3.0946e-01, -1.0074e-01,\n",
      "          -6.8404e-01, -2.2661e-01,  3.3777e-01,  5.9691e-01,  5.9284e-01,\n",
      "           2.8411e-02,  7.1868e-01,  6.8228e-01, -1.5033e-01, -1.2509e-01,\n",
      "           1.1662e-01]],\n",
      "\n",
      "        [[-1.7653e-01,  8.8109e-03, -2.6314e-01, -2.6559e-01,  3.0104e-01,\n",
      "          -4.4601e-01, -2.1885e-01,  5.0812e-01, -1.0958e-01,  1.1087e-01,\n",
      "           1.2304e-01,  6.9562e-01,  1.9579e-01, -1.3873e-02, -1.2945e-01,\n",
      "           4.7606e-01],\n",
      "         [-2.2091e-01, -8.8794e-02, -1.1072e-01, -2.9727e-01,  2.4947e-01,\n",
      "          -3.6491e-01, -1.5197e-01,  5.2034e-01, -1.6230e-02,  7.3603e-02,\n",
      "           1.7947e-01,  5.8004e-01,  2.2844e-01, -8.1624e-02, -2.2250e-01,\n",
      "           4.1826e-01],\n",
      "         [-2.4097e-01, -8.0682e-02, -1.8234e-01, -2.9289e-01,  2.2566e-01,\n",
      "          -3.8954e-01, -1.1939e-01,  5.5760e-01, -3.5645e-02,  6.1902e-02,\n",
      "           9.7404e-02,  6.2825e-01,  1.9248e-01, -3.6235e-02, -1.8128e-01,\n",
      "           4.0453e-01],\n",
      "         [-2.1263e-01, -6.1012e-02, -2.0884e-01, -2.8454e-01,  3.0532e-01,\n",
      "          -3.7339e-01, -2.3984e-01,  5.9957e-01, -6.2526e-02,  1.1584e-01,\n",
      "           8.4179e-02,  6.2378e-01,  2.2790e-01, -8.5415e-02, -2.0233e-01,\n",
      "           4.4104e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 0.1141,  0.1785,  0.0131,  0.3057, -0.2007,  1.2946,  0.9211,\n",
      "           0.3265,  0.9042,  1.0670, -1.5717, -0.7287,  0.3936, -2.6306,\n",
      "          -0.9293,  0.5424],\n",
      "         [-0.5400,  0.4761,  0.5886,  0.1282,  0.3549,  0.7022,  1.1494,\n",
      "           2.5192,  0.2201, -0.3523,  0.2181, -1.5305, -1.0345, -1.0993,\n",
      "          -1.3306, -0.4696],\n",
      "         [ 1.3431,  0.4197,  1.0427, -1.1045, -0.5771,  0.7826,  0.5608,\n",
      "          -0.4944, -1.7668,  0.9593,  0.9504,  0.3086, -0.6681, -1.6351,\n",
      "           0.9496, -1.0706],\n",
      "         [ 0.3183, -1.3204, -0.7837,  1.8534, -0.0522, -1.0064,  1.0298,\n",
      "          -1.0653, -1.4799,  1.0242, -0.8385, -0.5091,  0.9919,  0.2349,\n",
      "           0.3711,  1.2317]],\n",
      "\n",
      "        [[-0.4410,  1.1942, -0.9669,  0.1602, -0.7747,  2.1991,  0.4440,\n",
      "          -0.3265, -0.8037, -1.5178,  0.8924, -1.1214, -0.9704,  0.2138,\n",
      "           0.7779,  1.0407],\n",
      "         [ 0.2050,  1.2679,  0.9811, -1.4350,  1.5426,  0.0334,  0.5895,\n",
      "           1.2421, -1.4152, -1.6493, -0.2959,  0.0136, -1.1340,  0.5757,\n",
      "          -0.7575,  0.2360],\n",
      "         [-0.4222,  0.4057,  0.3401, -0.8354, -2.4040,  1.3099,  0.7077,\n",
      "          -1.2478,  0.1671, -1.3088, -0.1609,  1.2454,  1.1112,  0.7793,\n",
      "          -0.0456,  0.3586],\n",
      "         [-0.9318, -0.2664, -0.0597,  2.1121, -0.9104, -0.6987, -0.9118,\n",
      "          -1.3991,  1.3558,  1.4829,  0.5708,  0.5161, -0.5833,  0.0567,\n",
      "          -1.0216,  0.6884]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 9.4629e-01, -8.6549e-01, -2.1239e+00,  9.7739e-01,  8.2042e-01,\n",
      "          -1.0132e+00,  7.9091e-01,  2.2539e-01, -8.1500e-01,  3.4218e-02,\n",
      "           9.5298e-01,  6.1854e-01, -1.4124e+00, -6.2389e-01,  1.4740e+00,\n",
      "           1.3802e-02],\n",
      "         [ 2.9195e-01, -2.2397e+00,  8.8113e-01, -3.0781e-01, -4.5435e-01,\n",
      "           8.5958e-01, -3.6618e-01,  7.4811e-01,  9.1870e-01,  3.1879e-01,\n",
      "          -1.0885e+00,  8.8177e-01, -1.5821e+00,  1.2009e+00,  8.4443e-01,\n",
      "          -9.0677e-01],\n",
      "         [ 6.2981e-01,  1.0584e+00, -9.7113e-01, -1.1968e+00,  3.7083e-01,\n",
      "          -2.8831e-01, -2.3384e-03,  1.0949e+00, -5.4055e-01, -9.5516e-01,\n",
      "          -3.0232e-01,  3.8559e-01, -2.1090e-01, -3.4646e-01, -1.3959e+00,\n",
      "           2.6703e+00],\n",
      "         [ 1.0812e-01, -8.9144e-01,  1.0760e+00,  3.6180e-01,  8.6733e-01,\n",
      "          -5.0667e-01,  3.2318e-02,  7.3347e-01, -1.8318e+00, -6.9435e-01,\n",
      "          -2.0644e+00,  1.1195e+00,  9.1380e-01,  1.3014e+00,  1.4829e-01,\n",
      "          -6.7343e-01]],\n",
      "\n",
      "        [[-9.6262e-01, -1.4634e+00,  6.2009e-01, -1.3696e+00, -7.1977e-01,\n",
      "          -1.4020e+00, -9.1345e-02, -1.3584e-01, -2.3429e-01,  1.0185e+00,\n",
      "           9.2473e-01,  1.6477e+00,  6.4827e-02, -2.8676e-01,  1.7225e+00,\n",
      "           6.6735e-01],\n",
      "         [ 4.6891e-01, -1.0077e+00,  1.2547e+00,  9.6948e-01,  5.3927e-02,\n",
      "           3.5578e-01,  1.0504e+00,  1.6346e+00, -1.0785e-01, -4.3758e-01,\n",
      "          -2.0280e-01,  8.1490e-03, -1.9806e+00,  1.6330e-01, -2.0283e+00,\n",
      "          -1.9448e-01],\n",
      "         [-1.4585e+00, -8.6238e-01, -2.9674e-01, -1.4407e+00, -2.8427e-01,\n",
      "           1.2982e+00,  4.0065e-01,  9.7792e-02,  1.3062e+00,  6.4780e-02,\n",
      "          -3.3934e-01,  1.6820e+00, -2.9761e-01,  1.1554e+00,  5.8848e-01,\n",
      "          -1.6139e+00],\n",
      "         [-6.7496e-01,  4.8698e-01,  7.3344e-01,  1.4566e+00,  3.8063e-01,\n",
      "          -1.0594e+00,  9.2809e-01,  1.3751e-01, -5.5650e-01, -2.5741e+00,\n",
      "           8.0790e-01, -4.3680e-01, -4.1556e-01,  1.0907e+00, -9.6223e-01,\n",
      "           6.5766e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-1.5145e-01, -1.7835e+00,  1.5911e+00, -1.1305e+00, -2.0248e-01,\n",
      "          -1.7236e-01, -2.5691e-01,  1.0639e-01,  1.3678e-01, -1.3581e+00,\n",
      "          -1.9541e+00,  1.0692e+00,  2.5632e-01,  1.5851e+00, -3.5327e-01,\n",
      "          -7.6948e-01],\n",
      "         [-2.6897e-01,  3.3643e-03,  6.7877e-01, -1.8060e+00,  1.1221e+00,\n",
      "           7.3462e-01, -6.2694e-01, -5.8973e-01, -7.8161e-01,  4.1581e-01,\n",
      "           2.2587e+00, -9.8660e-01,  6.9652e-01, -2.2425e+00,  1.2831e+00,\n",
      "           3.3738e-01],\n",
      "         [-3.8838e-01,  9.6339e-01, -1.5194e+00, -1.8377e+00, -6.2674e-01,\n",
      "           9.8968e-01,  1.6824e+00,  8.7743e-01, -2.3647e-01,  9.5524e-01,\n",
      "           2.0287e+00, -8.7897e-01, -6.1398e-01,  1.2012e+00,  2.1610e-01,\n",
      "           6.5142e-01],\n",
      "         [-1.7619e+00, -9.4223e-01, -1.8241e+00, -1.4537e+00,  4.1350e-01,\n",
      "           8.4991e-01, -4.0169e-01,  1.9184e+00, -5.3129e-01, -5.0056e-01,\n",
      "          -4.7706e-03,  1.9458e-01, -1.6724e+00,  1.1802e+00,  6.7215e-01,\n",
      "          -4.2412e-01]],\n",
      "\n",
      "        [[-1.7090e+00,  3.0445e-01, -9.8089e-01,  7.6308e-01,  1.7345e-01,\n",
      "           9.7750e-01, -6.4510e-01,  8.1102e-01,  5.4479e-01,  1.5661e-01,\n",
      "          -8.0608e-01,  4.8863e-01, -3.4148e-01,  8.5241e-01,  2.2842e+00,\n",
      "           5.9338e-01],\n",
      "         [-4.0047e+00,  2.7875e-01,  1.8962e+00, -1.2846e+00, -3.1785e-01,\n",
      "          -1.6304e+00, -3.4106e-01, -2.9647e+00, -2.8352e+00, -5.7978e-01,\n",
      "           1.5011e+00, -8.6131e-01,  6.4428e-01, -2.8564e+00,  1.3631e+00,\n",
      "           1.3757e+00],\n",
      "         [ 1.8575e+00,  1.1961e+00, -3.7300e-01, -6.6126e-01, -9.8079e-01,\n",
      "           3.4943e-01, -2.0536e+00, -1.9306e-01,  7.9761e-01,  2.9102e-01,\n",
      "           3.8609e-01, -1.6267e+00,  1.1903e+00, -5.7724e-01, -1.0073e+00,\n",
      "          -3.6034e-01],\n",
      "         [-1.1674e+00, -1.9324e+00, -1.0260e+00, -1.3960e+00, -1.3407e+00,\n",
      "          -1.0050e+00, -1.7450e-01, -3.2437e-04, -9.4803e-01, -5.4841e-01,\n",
      "          -7.8352e-01,  1.0897e+00,  4.0842e-01,  9.7204e-02,  1.0465e+00,\n",
      "          -8.4238e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 5., 3.],\n",
       "        [2., 0., 4., 0.],\n",
       "        [7., 3., 1., 3.],\n",
       "        [4., 8., 3., 3.],\n",
       "        [2., 0., 9., 7.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 1],\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [0, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1, 4])\n",
      "tensor([[[[1, 0, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2035,  0.4429,  0.2635,  1.3967, -1.6652,  0.1203,  1.7866,\n",
      "           0.8812,  0.0507,  0.1804, -0.0381, -2.3824, -0.2068,  0.1513,\n",
      "          -1.0956, -0.0891],\n",
      "         [ 1.1244,  1.0945, -0.0246,  1.2299, -0.8733, -1.7951,  2.0712,\n",
      "          -0.4332, -0.5497, -1.2233, -0.7493, -0.0234, -0.7032, -0.2260,\n",
      "           0.3518,  0.7292],\n",
      "         [ 0.6582,  1.6080,  0.0794,  1.7047, -1.0895, -0.9417,  1.1758,\n",
      "          -1.6279,  0.1364, -1.5671,  0.7436,  0.5458, -0.2716, -0.5609,\n",
      "          -0.1057, -0.4876],\n",
      "         [ 1.4413, -0.0748, -0.2107,  1.3109, -1.3763, -0.3701,  2.0483,\n",
      "          -0.2628, -0.7714, -1.1779,  0.9788, -0.0193, -1.5405, -0.2082,\n",
      "           0.6033, -0.3705]],\n",
      "\n",
      "        [[ 1.9578,  0.8033, -0.3147,  1.4998, -0.2427, -0.6373, -0.4881,\n",
      "          -1.3814,  0.6539, -1.1561, -0.9697, -0.6097,  0.3018,  1.4751,\n",
      "           0.1945, -1.0865],\n",
      "         [ 1.7017,  0.4597,  1.2454,  1.3109, -1.2802, -1.8012,  1.2904,\n",
      "          -1.0481, -0.3061, -0.8629,  0.3082,  0.3714, -0.0702, -0.3868,\n",
      "          -0.0683, -0.8638],\n",
      "         [ 2.1357,  1.0637,  0.7439, -0.5668, -0.8651, -1.0720, -0.6719,\n",
      "          -0.8829,  0.6423, -1.3971, -0.0452, -0.5051,  1.4962,  0.2882,\n",
      "          -0.9865,  0.6225],\n",
      "         [ 2.2542,  0.3034,  1.2670,  1.2373, -1.0141, -1.5629,  0.2357,\n",
      "          -1.5373, -0.3597, -0.5716,  0.5606,  0.3093, -0.1590,  0.2712,\n",
      "          -0.8093, -0.4248]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    causal_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = causal_mask(\n",
    "    tgt_len=4,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    tgt_len=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "      (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.8505e-01, -1.2715e-01,  9.2629e-02,  ...,  1.5001e-02,\n",
      "           2.9675e-02,  1.2091e-03],\n",
      "         [-1.2576e-01, -4.6871e-02,  2.9593e-02,  ...,  2.3546e-02,\n",
      "           2.5722e-02, -6.3521e-03],\n",
      "         [-1.7734e-01, -6.3951e-02,  4.7980e-02,  ...,  7.0407e-02,\n",
      "           1.1833e-01, -5.9716e-03],\n",
      "         [ 3.2495e-02,  6.2671e-02,  7.6747e-02,  ..., -1.9443e-04,\n",
      "          -2.0474e-02,  4.3179e-02]],\n",
      "\n",
      "        [[-1.8734e-01, -1.7068e-01,  8.8445e-02,  ...,  7.2931e-03,\n",
      "           1.7551e-02,  8.9000e-03],\n",
      "         [-1.4641e-02, -2.7307e-02, -2.0989e-02,  ...,  6.3315e-02,\n",
      "           3.1620e-02, -7.7127e-02],\n",
      "         [-6.0200e-02, -1.8704e-02,  5.8235e-02,  ..., -8.2347e-05,\n",
      "           9.7401e-02, -1.1445e-01],\n",
      "         [-9.4918e-02,  1.4912e-03,  9.7382e-02,  ...,  2.1650e-02,\n",
      "           7.2902e-02,  4.9981e-04]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1811, -1.9758, -0.0209, -0.2662,  0.3135,  0.1960,  0.4113,\n",
      "          -1.3872,  0.0121,  2.5192,  0.1217,  0.0626, -0.0180, -1.4118,\n",
      "           0.0753,  0.1872],\n",
      "         [-0.5383, -1.8345, -0.0548, -0.3420, -0.1049,  1.9727,  0.2111,\n",
      "          -0.9605,  0.3124,  0.7549, -0.0456,  0.9636, -0.7188, -0.5888,\n",
      "          -0.9998,  1.9734],\n",
      "         [-0.0276,  1.0978,  0.4478, -0.2224, -0.2727, -0.5881,  0.1477,\n",
      "          -0.2370,  1.7011, -1.4158,  1.4406, -0.0437,  0.0855, -0.0873,\n",
      "          -2.5946,  0.5688],\n",
      "         [-1.6403,  0.3979, -0.9926,  1.8256, -1.5456, -0.1903,  0.3617,\n",
      "           0.9597,  0.2037,  1.8273,  0.6445, -1.0395, -0.1657, -0.4575,\n",
      "           0.2406, -0.4295]],\n",
      "\n",
      "        [[ 1.2429, -1.5708, -0.0187, -0.7662, -0.3011, -1.1549, -0.0059,\n",
      "           0.3748,  0.1355,  2.8128, -0.1528,  0.2096, -1.3098,  0.2375,\n",
      "           0.2569,  0.0104],\n",
      "         [-0.6852, -2.0153,  0.6160, -0.2926, -0.2164,  1.8641,  0.3189,\n",
      "          -0.9426,  0.2776,  0.6464, -0.0851,  0.9074, -0.5471, -0.6519,\n",
      "          -1.0321,  1.8377],\n",
      "         [-1.6490, -0.9326, -1.1304,  1.8379, -0.7703, -0.3154,  0.7437,\n",
      "           0.6175,  0.7753, -0.0645,  0.6856, -0.4125, -0.8400, -0.1289,\n",
      "           1.9973, -0.4136],\n",
      "         [-1.4770,  0.1190, -0.8592,  1.3633, -1.5379, -0.3486,  0.7113,\n",
      "           0.5674, -0.0249,  1.2252,  0.1979, -1.2040, -0.3942,  0.1281,\n",
      "           2.1000, -0.5664]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7030,  1.2238, -0.8282, -1.2306,  0.2240,  0.9211,  0.4974,\n",
      "           1.2587, -1.6217,  1.9507, -1.3152,  0.4386, -0.6237, -0.4183,\n",
      "           0.3690, -0.1425],\n",
      "         [-0.4232, -0.9888, -0.7269,  0.3062, -0.0348, -0.1346,  1.5107,\n",
      "           0.3886,  0.2488,  2.5285,  0.0671, -0.3590, -0.0501, -2.1834,\n",
      "          -0.5211,  0.3719],\n",
      "         [-0.2952, -0.2299, -0.4014, -0.0257,  0.1387,  0.2825,  1.8230,\n",
      "           1.2164, -0.8336,  1.1982, -0.8318, -0.9818, -0.2566, -2.1929,\n",
      "          -0.0693,  1.4595],\n",
      "         [ 0.8839, -0.5790,  0.6680,  1.1558, -0.5407, -0.0279,  0.0781,\n",
      "           0.0264,  0.9484, -0.9563,  2.1016, -1.2968,  0.3324, -0.9250,\n",
      "           0.1088, -1.9777]],\n",
      "\n",
      "        [[-0.7397,  1.2623, -0.9339, -1.2402, -0.0048,  1.0717,  0.5344,\n",
      "           1.3726, -1.5386,  1.7859, -1.2248,  0.5812, -0.6472, -0.4405,\n",
      "           0.3155, -0.1540],\n",
      "         [-0.9507,  1.2446, -2.0122, -0.5425, -0.5806, -0.4437,  0.4546,\n",
      "          -0.2613,  0.9666,  2.3420,  0.6610, -0.1226,  0.1053, -0.4385,\n",
      "          -0.9956,  0.5736],\n",
      "         [-0.2170,  0.8786,  1.5414, -0.4235, -1.6937, -0.0875,  1.2425,\n",
      "          -0.0725,  0.4062,  0.4758,  0.1380, -0.9437,  1.3811, -1.7941,\n",
      "          -1.2605,  0.4290],\n",
      "         [ 0.9420,  0.0874, -0.2795,  0.8690, -0.6526, -0.0586,  1.7403,\n",
      "           0.9530,  0.9794, -1.4506,  0.2591, -2.2782,  0.1193, -1.1546,\n",
      "           0.3049, -0.3803]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50265])\n",
      "tensor([[[ 0.0496, -0.1079, -0.0167,  ..., -0.0198, -0.0386,  0.0525],\n",
      "         [ 0.1125, -0.0119, -0.0858,  ..., -0.0611,  0.0310,  0.0009],\n",
      "         [ 0.0081, -0.0973,  0.0686,  ...,  0.0192, -0.0870,  0.0124],\n",
      "         [ 0.0875, -0.0125,  0.0058,  ...,  0.0281,  0.0241, -0.0837]],\n",
      "\n",
      "        [[-0.0566, -0.1057, -0.0058,  ...,  0.0238, -0.0864, -0.0197],\n",
      "         [ 0.1522,  0.0867, -0.0534,  ..., -0.0267,  0.0549,  0.0049],\n",
      "         [ 0.0024, -0.1179,  0.0793,  ..., -0.0657,  0.0527,  0.0201],\n",
      "         [ 0.1627, -0.0217,  0.0145,  ...,  0.0594,  0.0535,  0.0286]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 5))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)\n",
    "logits = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
