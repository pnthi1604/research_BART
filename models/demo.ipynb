{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 0.2308, -0.0155,  0.2638, -0.1180,  0.3354, -0.0364, -0.0682,\n",
      "           0.3735, -0.1165,  0.0083, -0.1876,  0.1315,  0.1648, -0.0027,\n",
      "          -0.0652, -0.2345],\n",
      "         [ 0.3073,  0.2864, -0.0532, -0.0928,  0.1919, -0.0834, -0.1109,\n",
      "           0.0523, -0.1892, -0.0031, -0.0571,  0.3419,  0.0720, -0.2508,\n",
      "           0.0377,  0.0878],\n",
      "         [ 0.1847,  0.1438,  0.0450, -0.0207,  0.2000, -0.1546, -0.1281,\n",
      "           0.0928, -0.1076, -0.0481,  0.0883,  0.1438,  0.1582, -0.1793,\n",
      "           0.0156, -0.0324],\n",
      "         [ 0.4912, -0.0124,  0.2691, -0.1006,  0.2902,  0.3086,  0.0203,\n",
      "           0.4224, -0.2376,  0.2309, -0.3679,  0.0156,  0.0165,  0.0038,\n",
      "          -0.1013, -0.5336]],\n",
      "\n",
      "        [[ 0.4683, -0.3188,  0.2930, -0.0008,  0.2028,  0.0450, -0.1408,\n",
      "          -0.5044,  0.0619,  0.1134, -0.3225,  0.2031,  0.2396,  0.3058,\n",
      "          -0.3018, -0.0052],\n",
      "         [ 0.0198, -0.1136, -0.0749, -0.2530,  0.1354, -0.0514, -0.3659,\n",
      "          -0.0349,  0.4556,  0.3187, -0.1536,  0.0047, -0.0786, -0.0749,\n",
      "          -0.0507, -0.0707],\n",
      "         [ 0.3467,  0.0252,  0.1703, -0.0193,  0.0353,  0.3796,  0.0988,\n",
      "           0.1011, -0.1778, -0.1197, -0.4042,  0.2411,  0.3299, -0.1279,\n",
      "          -0.0681, -0.2864],\n",
      "         [ 0.2455,  0.1296, -0.0947, -0.0659, -0.0729,  0.0073, -0.1645,\n",
      "           0.1323,  0.0848,  0.0849, -0.1418,  0.0373, -0.0411, -0.2079,\n",
      "           0.1835, -0.1091]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-1.0815,  1.4600,  0.9880, -0.0311, -1.0488, -1.8177,  0.9473,\n",
      "          -0.5387,  0.0030,  0.6707,  0.8261,  0.7633,  1.1093, -0.9392,\n",
      "          -1.5049,  0.1941],\n",
      "         [ 0.4348,  0.5269, -0.2812,  0.6041,  1.5712,  0.5017, -0.6394,\n",
      "           0.8311,  1.1606, -1.0152, -1.0899, -0.7408, -1.1341, -1.9703,\n",
      "          -0.1601,  1.4005],\n",
      "         [-1.6643, -1.5625,  0.8132,  0.6971, -1.1672,  0.1583, -1.2897,\n",
      "           0.7968, -0.4668,  0.3510,  1.3909,  1.4903,  1.0344, -0.0458,\n",
      "          -0.6166,  0.0809],\n",
      "         [ 0.8642,  0.3762,  0.4719, -2.0107,  1.0089, -0.5401, -0.4147,\n",
      "           1.6356, -1.1949,  1.2460, -1.1774, -0.0626,  0.5106, -1.3265,\n",
      "           0.5415,  0.0719]],\n",
      "\n",
      "        [[ 0.3826, -1.4632,  0.5039,  0.1636,  1.9924,  0.5149,  1.0296,\n",
      "          -1.0267, -0.5357, -0.2586, -1.5328,  0.1074, -0.6634,  0.7827,\n",
      "           1.2892, -1.2858],\n",
      "         [ 1.5613,  1.8288,  0.4619,  0.0991,  1.5021, -0.6401,  0.8571,\n",
      "          -0.0089, -1.4944, -0.0849, -0.7845,  0.3137, -0.5040, -1.3426,\n",
      "          -0.9960, -0.7687],\n",
      "         [-0.6838,  1.5133,  0.3192,  1.4607,  0.2484,  0.0510, -0.8386,\n",
      "           0.6354, -0.2704, -1.0484, -0.7051, -0.3245, -0.6203, -2.1701,\n",
      "           1.2713,  1.1620],\n",
      "         [ 1.0865, -1.7604, -0.9394,  0.7155,  0.5490,  1.6414,  0.6641,\n",
      "          -0.4078,  0.1561,  0.7498, -1.7569, -1.2336, -0.6579,  0.3818,\n",
      "           0.9626, -0.1506]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-7.5790e-02, -1.1394e+00,  4.7269e-01, -1.2430e+00,  1.3378e+00,\n",
      "          -1.1735e+00,  1.3739e+00, -8.2089e-01,  1.3069e+00, -4.6396e-01,\n",
      "          -8.7848e-01,  6.6484e-02, -1.3064e+00,  3.0947e-01,  1.4343e+00,\n",
      "           7.9987e-01],\n",
      "         [-1.9415e+00, -5.7180e-01, -3.8017e-02, -1.2561e-01, -4.0291e-01,\n",
      "           1.0254e-01,  1.9303e+00,  1.3619e+00,  1.2873e+00, -7.2652e-01,\n",
      "           4.1702e-01, -1.3988e+00, -1.0389e+00,  6.3991e-01,  5.6920e-01,\n",
      "          -6.4150e-02],\n",
      "         [ 1.6729e+00,  1.5638e+00,  1.3269e-01, -1.3936e-03, -1.0760e+00,\n",
      "          -1.7465e+00,  2.1962e-01,  4.6269e-01, -5.7551e-01, -5.3958e-01,\n",
      "          -4.6256e-01,  1.4280e+00,  8.6421e-01,  1.9205e-01, -6.6056e-01,\n",
      "          -1.4739e+00],\n",
      "         [-7.3808e-01, -8.1106e-01, -9.8833e-01, -4.1538e-01,  9.2842e-01,\n",
      "           2.2284e+00,  3.2173e-01,  1.0823e+00,  1.1188e+00, -9.5381e-01,\n",
      "           2.1273e-01, -7.9885e-01, -1.6523e+00,  9.2900e-01, -2.3064e-01,\n",
      "          -2.3280e-01]],\n",
      "\n",
      "        [[ 1.0224e+00,  7.6958e-01,  2.3345e+00, -7.3317e-01, -1.1940e-01,\n",
      "          -1.2408e+00,  8.4576e-01, -1.2750e+00,  3.9214e-01,  4.8369e-02,\n",
      "          -2.9533e-01, -5.0322e-01, -1.6419e+00,  1.8224e-01,  8.9174e-01,\n",
      "          -6.7786e-01],\n",
      "         [-1.9769e+00, -8.8664e-01,  1.1025e+00,  1.3778e+00, -6.8435e-03,\n",
      "           4.0273e-01, -5.3004e-01, -1.7853e+00,  1.2853e+00,  6.6034e-01,\n",
      "          -4.7755e-01,  8.4653e-02,  2.9155e-01, -1.0244e+00,  6.9647e-01,\n",
      "           7.8631e-01],\n",
      "         [ 3.1632e-01,  4.7366e-01,  1.0891e+00, -1.2260e+00, -7.6546e-01,\n",
      "           1.4394e+00, -1.7673e-01, -5.7466e-01,  1.2211e+00, -9.1990e-01,\n",
      "          -1.4954e+00,  8.9040e-01, -3.8457e-01,  1.7595e+00, -8.4701e-01,\n",
      "          -7.9974e-01],\n",
      "         [ 2.0341e+00,  6.7119e-04, -1.0730e-01, -2.2909e+00,  7.3035e-02,\n",
      "          -7.3017e-02, -2.9647e-01, -1.2395e+00,  5.8902e-01,  9.0131e-01,\n",
      "          -6.6009e-01,  1.6848e+00,  1.3426e-01,  1.4173e-01, -2.2423e-01,\n",
      "          -6.6736e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-0.9708,  0.7682, -1.5229,  0.2895, -1.4604,  2.7455,  0.5427,\n",
      "          -2.5780,  0.4655,  1.6350, -1.0364,  2.2142, -1.3534,  1.4662,\n",
      "          -2.0265, -1.0402],\n",
      "         [-0.9817,  1.2918, -2.0837,  0.3518, -0.0745,  0.7030, -1.6515,\n",
      "          -1.7057,  0.1480, -2.9250,  0.2276, -1.4785,  1.3636, -0.6095,\n",
      "           0.0053,  1.0012],\n",
      "         [-1.9733,  1.3933,  0.3711, -2.4373, -1.0679,  1.3285,  0.2691,\n",
      "           0.7369, -0.0702, -0.4021,  0.6217,  1.3696, -0.7926, -0.6448,\n",
      "          -0.0437, -1.8793],\n",
      "         [-1.6146, -0.2036, -1.6556, -0.2937,  0.7043,  2.1998, -0.8551,\n",
      "          -3.7813, -3.0618,  0.3897, -1.8210,  0.7424, -0.7821,  2.9827,\n",
      "           0.6485,  0.6580]],\n",
      "\n",
      "        [[-2.1263, -0.9626, -2.9456, -0.2003, -2.9131, -0.4630,  0.3145,\n",
      "           0.3150,  0.0372,  1.7001, -1.1304,  0.6329, -0.2938,  1.1666,\n",
      "          -1.9255, -3.2452],\n",
      "         [-0.5003, -0.3817, -2.0883,  0.1109,  0.1218,  0.4592,  1.4281,\n",
      "          -0.3805, -0.9034, -1.0312, -0.7936, -3.1307,  0.7696, -2.7581,\n",
      "          -1.9488, -2.2312],\n",
      "         [-0.0466,  2.3635,  0.0290,  1.0093, -1.1305, -0.3025, -2.0163,\n",
      "           1.3089, -0.0974,  1.1734, -1.4314,  0.2573, -0.4114,  1.9028,\n",
      "          -0.1994,  0.1178],\n",
      "         [-2.6490,  0.7850, -1.7739,  0.6580,  0.0245, -0.0629, -0.5998,\n",
      "          -0.8340, -2.9169,  0.8873, -0.0041, -0.0697,  0.8117,  2.8645,\n",
      "           0.8483,  1.0443]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 6., 5., 0.],\n",
       "        [4., 2., 3., 8.],\n",
       "        [1., 2., 4., 6.],\n",
       "        [1., 3., 0., 2.],\n",
       "        [2., 1., 0., 6.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 0, 1, 1],\n",
       "        [1, 0, 1, 1],\n",
       "        [1, 1, 1, 0],\n",
       "        [0, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1, 4])\n",
      "tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      " attention_mask = tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]])\n",
      " attention_mask.shape = torch.Size([2, 1, 1, 4])\n",
      " attn_weights.shape = torch.Size([2, 16, 4, 4])\n",
      "tensor([[[ 0.3846,  1.8040, -0.8200, -0.7739,  1.6684,  0.1610, -0.5116,\n",
      "          -1.3416,  0.1045, -0.6137, -0.7870,  0.1949, -0.8089, -0.8514,\n",
      "           1.9985,  0.1922],\n",
      "         [-0.5217,  0.2680, -1.4556,  0.6203,  0.3232, -0.3569, -1.5752,\n",
      "           1.3192,  0.1439, -0.5357,  0.1467, -2.0769,  0.3098,  1.2145,\n",
      "           1.3299,  0.8466],\n",
      "         [-1.2964, -0.9452,  1.3790,  0.4783, -0.4113, -0.3256, -0.9141,\n",
      "           1.3571,  0.2883,  0.5737, -1.8338, -0.7339, -0.1045,  0.3036,\n",
      "           1.9698,  0.2149],\n",
      "         [-0.0701,  0.2262,  0.5714, -1.4173,  1.3314, -0.6460, -1.7927,\n",
      "           1.5200,  0.3362, -1.3614,  0.3573, -1.4410,  0.4566,  0.2758,\n",
      "           1.0165,  0.6372]],\n",
      "\n",
      "        [[ 1.1604, -1.3240, -0.9973, -1.3264, -0.1416,  0.2381, -0.6038,\n",
      "           0.3174,  0.5563,  1.0254, -1.3720, -0.7404,  2.2613,  0.7950,\n",
      "           0.2427, -0.0911],\n",
      "         [ 0.9773, -1.5056,  0.1857, -1.3571, -0.1688, -0.0496, -0.8171,\n",
      "           2.1165,  0.3059,  0.2236, -1.5120, -0.2819,  1.6971,  0.5141,\n",
      "          -0.0738, -0.2544],\n",
      "         [ 0.9259, -1.3202, -0.8587, -1.2042, -0.1525,  0.0689, -0.8646,\n",
      "           1.9749,  0.3265,  0.7600, -1.2902, -0.6872,  1.8618,  0.4934,\n",
      "          -0.0667,  0.0329],\n",
      "         [ 0.9337, -1.3180, -0.8553, -1.2017, -0.1473,  0.0745, -0.8613,\n",
      "           1.9853,  0.3328,  0.7674, -1.2879, -0.6834,  1.8720,  0.4122,\n",
      "          -0.0613,  0.0385]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    causal_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = causal_mask(\n",
    "    tgt_len=4,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expand_mask() got an unexpected keyword argument 'tgt_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      3\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcreate_decoder_atn_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research_BART/models/bart_model_from_scratch/utils/mask.py:33\u001b[0m, in \u001b[0;36mcreate_decoder_atn_mask\u001b[0;34m(attention_mask, tgt_len)\u001b[0m\n\u001b[1;32m     28\u001b[0m     tgt_len \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m causal_4d_mask \u001b[38;5;241m=\u001b[39m causal_mask(\n\u001b[1;32m     30\u001b[0m     tgt_len\u001b[38;5;241m=\u001b[39mtgt_len,\n\u001b[1;32m     31\u001b[0m     device\u001b[38;5;241m=\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m expanded_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mexpand_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expanded_attn_mask \u001b[38;5;241m&\u001b[39m causal_4d_mask\n",
      "\u001b[0;31mTypeError\u001b[0m: expand_mask() got an unexpected keyword argument 'tgt_len'"
     ]
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    tgt_len=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 5))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)\n",
    "logits = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
