{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-0.0906, -0.1523,  0.0645, -0.0779, -0.4009,  0.1179, -0.0401,\n",
      "          -0.0256,  0.4397, -0.4632, -0.3262, -0.3941, -0.0053,  0.2744,\n",
      "           0.5118, -0.2259],\n",
      "         [ 0.3221,  0.0747,  0.3362, -0.1945, -0.0732,  0.0699,  0.0223,\n",
      "           0.0306,  0.0310, -0.5592, -0.1614, -0.2145,  0.2070,  0.3759,\n",
      "           0.3163, -0.5995],\n",
      "         [ 0.6112,  0.1720,  0.2572, -0.2552,  0.0168, -0.0536, -0.3061,\n",
      "           0.2608, -0.1535, -0.5237,  0.0262, -0.0588,  0.6172,  0.4804,\n",
      "          -0.0105, -0.9807],\n",
      "         [ 0.8390,  0.3498,  1.0678,  0.0452, -0.7960,  0.6988, -0.3431,\n",
      "          -0.7610,  0.4373, -0.0960, -0.4153, -0.3340,  0.0728, -0.1029,\n",
      "           0.7400, -0.4680]],\n",
      "\n",
      "        [[-0.2008,  0.2546, -0.2252,  0.0403, -0.0884, -0.0938, -0.0812,\n",
      "           0.1076, -0.0319,  0.1269,  0.0424,  0.1358,  0.2720,  0.2933,\n",
      "          -0.0975, -0.1525],\n",
      "         [ 0.0311,  0.1533,  0.0352,  0.0074, -0.1858, -0.0321,  0.0199,\n",
      "           0.0076, -0.1418,  0.1541,  0.1712,  0.3522,  0.0963,  0.3625,\n",
      "          -0.1823, -0.3024],\n",
      "         [-0.1899, -0.0389, -0.2038,  0.1288, -0.2213, -0.0066,  0.3060,\n",
      "          -0.2084,  0.0606, -0.1316,  0.0705,  0.2057, -0.0131,  0.3134,\n",
      "           0.1669, -0.1196],\n",
      "         [-0.1377, -0.0175, -0.1888,  0.0612, -0.2572, -0.0082,  0.1671,\n",
      "          -0.0976, -0.1639, -0.0149,  0.0618,  0.1853,  0.1379,  0.3834,\n",
      "          -0.0572, -0.3141]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 0.8866,  0.9529, -0.0799, -0.1191, -0.5029, -0.0234,  0.2104,\n",
      "          -1.3184, -0.4672,  1.9999, -1.6847,  2.0458, -0.2491, -0.4718,\n",
      "          -0.4713, -0.7077],\n",
      "         [ 0.4436,  0.9698, -0.5733, -0.3309,  0.1695, -0.5152, -0.6032,\n",
      "          -0.9839,  1.3384, -1.5008,  0.7942, -2.0934,  0.7560, -0.2112,\n",
      "           1.6097,  0.7308],\n",
      "         [-0.2578, -0.4657, -0.6380, -0.7219, -0.2121,  0.1674, -1.7863,\n",
      "          -1.0285,  0.8287,  0.7400,  1.3973, -0.7108, -1.0396,  1.0719,\n",
      "           0.6133,  2.0418],\n",
      "         [ 0.3813, -0.3535,  0.9814,  0.4377,  0.1263,  1.8616,  0.5039,\n",
      "           0.5770, -0.4403, -1.8439,  0.5302, -0.3310, -0.3050, -2.5194,\n",
      "           0.1538,  0.2400]],\n",
      "\n",
      "        [[ 0.0121, -1.0717, -0.9269, -1.3595,  1.7191, -0.7353,  0.3082,\n",
      "           1.0552,  1.0707,  0.7819, -0.9308,  0.7366, -0.4859, -1.5454,\n",
      "           0.0911,  1.2806],\n",
      "         [ 0.8386, -1.0170, -1.2013, -0.4672, -1.1312,  0.4353, -2.2052,\n",
      "          -0.2883,  1.8104,  1.4753,  0.1417,  0.3066,  0.6018,  0.3968,\n",
      "           0.3134, -0.0099],\n",
      "         [-2.0679, -0.2763,  0.4814,  0.8674, -0.3444, -0.0178,  1.1857,\n",
      "          -1.6905, -0.4543,  0.2911,  1.6887,  0.9654,  0.1827, -0.5096,\n",
      "           0.8156, -1.1172],\n",
      "         [-0.2106,  2.4120,  0.7499, -1.2776, -0.4082, -0.5065,  1.6692,\n",
      "          -0.0081, -1.1087, -1.0562,  0.4895, -0.2550, -1.0533, -0.4685,\n",
      "           0.8496,  0.1827]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-0.5961, -0.2438,  2.0707,  0.8528, -0.3247, -0.8230, -0.4278,\n",
      "           0.8240,  1.3356,  0.9072, -1.2335, -0.3611,  1.0598, -0.4635,\n",
      "          -1.2227, -1.3541],\n",
      "         [ 0.3346,  1.9473,  0.8060,  0.7508, -0.0184,  0.5457,  0.5893,\n",
      "           0.1226,  0.0605, -1.1706, -1.9169, -1.6469,  0.2156, -1.4014,\n",
      "           0.6602,  0.1216],\n",
      "         [-0.0885,  0.1404,  0.3864, -0.6493, -1.6866, -0.4397, -0.9270,\n",
      "           0.4149, -0.7006,  1.2153,  1.5563, -0.1757,  0.6654,  1.2475,\n",
      "          -1.9715,  1.0127],\n",
      "         [-1.0432,  1.1607, -2.1389, -0.8634, -0.8447, -0.1974, -0.2165,\n",
      "          -0.3523,  2.0631, -0.0163,  0.1888,  1.3780,  0.8251,  0.3682,\n",
      "          -0.5193,  0.2081]],\n",
      "\n",
      "        [[ 1.5042,  0.0541, -1.3730, -0.1065, -1.9605, -1.6317,  0.6744,\n",
      "           1.5118,  0.0288, -0.2980,  0.5462,  0.3287,  0.8157, -1.0040,\n",
      "           0.5197,  0.3900],\n",
      "         [ 0.4662,  1.3827, -0.2456,  1.7677, -1.0146, -0.7535, -1.5256,\n",
      "           1.4822, -0.7847, -0.5350, -1.2232,  0.7869, -0.3746, -0.0936,\n",
      "           1.0993, -0.4346],\n",
      "         [ 1.2549,  1.1386, -0.4714, -1.2206, -0.9920, -1.4816, -0.1912,\n",
      "           1.6118, -0.3196, -0.4532,  0.2415,  1.6723, -0.4574, -0.8446,\n",
      "           1.0889, -0.5765],\n",
      "         [ 0.4929,  2.1383,  1.2038, -0.5647, -1.8052, -0.7950, -0.8064,\n",
      "           0.5268, -0.0912,  0.3449,  0.6639, -0.1342, -0.9208, -1.4158,\n",
      "           0.0899,  1.0729]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 2.5787, -0.3854,  0.2765,  0.5187, -1.2923,  0.0393,  1.2051,\n",
      "           0.5166,  0.7347,  0.6127,  2.2282,  2.6512,  0.0838, -1.0159,\n",
      "           0.0713,  0.6154],\n",
      "         [-0.2097, -0.3558,  0.5705, -0.3296,  1.2369,  1.0185,  0.2950,\n",
      "           1.6750,  0.8265, -1.6832, -0.6825, -0.0399,  0.4249,  1.0549,\n",
      "           0.7891, -1.7902],\n",
      "         [ 0.6245, -1.6816, -0.1927,  1.0083, -0.0978,  0.1457, -0.8657,\n",
      "           0.1397, -1.0650,  0.9363, -0.6423,  0.1529, -1.0550,  1.1428,\n",
      "           0.9876, -1.2842],\n",
      "         [ 1.9195, -0.9235, -1.3548,  0.3458,  0.7945, -1.2909,  2.3783,\n",
      "          -1.6891, -0.8762, -1.5661, -1.6773, -2.1997,  0.6300, -1.0207,\n",
      "           1.1664,  0.9793]],\n",
      "\n",
      "        [[ 1.0350, -0.7139, -1.8564, -0.7620, -2.7044,  0.0135,  2.7190,\n",
      "          -0.7885,  1.2181, -0.3431,  0.5561,  1.7443,  2.7480,  0.2251,\n",
      "           1.1599,  1.6751],\n",
      "         [-0.1996,  2.8108, -0.0418,  2.4814,  0.1316, -0.9360,  2.0095,\n",
      "          -0.3664,  4.0593,  1.0269,  2.1967,  1.3186, -1.7025,  0.0131,\n",
      "           1.1636, -2.0571],\n",
      "         [-1.0874,  0.9545, -1.1404,  1.1318,  0.6856, -0.3696,  1.7639,\n",
      "           0.2150,  0.0528, -0.8753,  0.6095, -2.3660,  1.1691,  0.4464,\n",
      "           0.7656, -1.0324],\n",
      "         [ 0.0320,  0.4179, -3.9689,  1.2601,  2.2590, -2.0029,  0.4939,\n",
      "          -0.1850, -0.8166, -1.3491, -0.7328, -0.8321,  1.1074, -0.0535,\n",
      "           1.5694,  1.3505]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 8., 8., 2.],\n",
       "        [2., 6., 4., 6.],\n",
       "        [2., 5., 3., 7.],\n",
       "        [9., 3., 0., 4.],\n",
       "        [1., 4., 6., 3.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0],\n",
       "        [0, 1, 1, 1],\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1, 4])\n",
      "tensor([[[[1, 1, 1, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0892e+00, -5.1398e-01,  8.1492e-01, -1.2408e+00, -7.8178e-01,\n",
      "           3.8425e-01, -1.2705e-01,  1.3179e+00,  4.3272e-03,  5.0743e-01,\n",
      "          -3.0564e-01, -4.8269e-01, -1.6059e+00, -3.6146e-01, -1.0366e+00,\n",
      "           2.3379e+00],\n",
      "         [ 3.7795e-01,  1.6069e+00,  3.5906e-01, -8.9692e-01, -2.1237e+00,\n",
      "           9.7817e-01, -6.3405e-01,  1.0918e+00, -1.7468e+00,  1.7888e-01,\n",
      "           9.1982e-01, -3.7176e-01,  2.9797e-01, -3.3642e-01, -5.4547e-01,\n",
      "           8.4462e-01],\n",
      "         [ 5.9538e-01, -1.6238e+00,  1.7015e+00,  6.9254e-01, -1.4663e+00,\n",
      "          -5.2375e-01, -5.0907e-02, -5.4977e-01,  7.7115e-01,  3.7957e-01,\n",
      "          -6.6095e-02,  1.2309e+00,  8.0230e-02, -1.2828e-01, -1.9495e+00,\n",
      "           9.0717e-01],\n",
      "         [-7.0943e-01,  1.1993e+00,  2.4329e-01, -8.7603e-02,  1.3522e+00,\n",
      "           1.0797e-01, -2.1006e+00,  1.1015e+00,  3.1025e-02,  1.7022e-01,\n",
      "           3.4893e-01, -2.1934e+00,  6.7861e-01,  7.5636e-01, -7.0055e-01,\n",
      "          -1.9784e-01]],\n",
      "\n",
      "        [[-1.8423e-01,  5.9837e-01, -1.5101e+00,  1.5299e+00,  1.7661e+00,\n",
      "          -6.6800e-01,  3.4965e-01,  7.7504e-01, -1.3244e-01,  9.3088e-01,\n",
      "          -2.1768e+00,  2.8687e-01, -8.3158e-01, -2.0220e-01, -2.1070e-02,\n",
      "          -5.1038e-01],\n",
      "         [-3.0423e-01,  5.1092e-01, -1.4372e-01,  1.4398e+00,  1.9277e+00,\n",
      "          -8.5605e-01,  2.9394e-01,  8.3127e-01, -4.0325e-01,  8.6996e-01,\n",
      "          -2.4722e+00,  2.6164e-01, -7.7338e-01, -3.3157e-01, -1.4529e-01,\n",
      "          -7.0557e-01],\n",
      "         [-4.1866e-02,  6.6622e-01, -1.2831e+00,  1.6632e+00,  1.7193e+00,\n",
      "          -7.2139e-01, -4.0123e-02,  9.1759e-01, -5.9753e-02,  9.5737e-01,\n",
      "          -1.9129e+00,  4.5861e-01, -6.5353e-01,  3.1704e-02, -1.1305e+00,\n",
      "          -5.7089e-01],\n",
      "         [-4.8777e-02,  7.9451e-01, -1.7000e+00,  1.3108e+00,  1.6754e+00,\n",
      "          -6.8923e-01,  5.2885e-01,  9.9713e-01, -1.5070e-03,  9.2696e-01,\n",
      "          -1.8479e+00,  2.3837e-01, -5.9818e-01,  5.2738e-02, -1.1090e+00,\n",
      "          -5.3017e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    causal_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = causal_mask(\n",
    "    tgt_len=4,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    tgt_len=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "      (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0176, -0.0621, -0.0379,  ...,  0.0216,  0.0350, -0.0982],\n",
      "         [-0.0131, -0.0645,  0.0163,  ..., -0.0073,  0.0322, -0.1053],\n",
      "         [ 0.0045, -0.0684, -0.0128,  ...,  0.0121,  0.0448, -0.0999],\n",
      "         [ 0.0402, -0.0925,  0.0092,  ..., -0.0079,  0.0078, -0.0703]],\n",
      "\n",
      "        [[-0.0914, -0.0359, -0.0699,  ...,  0.0278,  0.0752,  0.0905],\n",
      "         [-0.0409,  0.0278, -0.0744,  ...,  0.0379,  0.0917,  0.0309],\n",
      "         [-0.0539,  0.0495,  0.0050,  ..., -0.0140,  0.1010,  0.0287],\n",
      "         [ 0.0660,  0.1194, -0.1428,  ...,  0.0897, -0.0701, -0.0951]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6533,  0.6205, -1.5151, -0.4721, -0.2612, -1.0028,  1.0264,\n",
      "           0.5035, -1.9236,  0.7580, -1.6013,  1.2098,  0.5314,  0.0932,\n",
      "           1.2997,  0.0803],\n",
      "         [ 0.6656,  0.6624, -1.5440, -0.1468, -0.1172, -1.0078,  1.0812,\n",
      "           0.5537, -1.9377,  0.0145, -1.6171,  1.3286,  0.5136,  0.1114,\n",
      "           1.3362,  0.1034],\n",
      "         [ 0.7470,  0.6833, -0.1467, -0.4984, -0.3193, -1.1431,  1.2286,\n",
      "           0.6032, -2.1538,  0.8538, -1.8473, -0.1103,  0.5894, -0.1431,\n",
      "           1.4955,  0.1611],\n",
      "         [ 0.6544,  0.6215, -1.5133, -0.4707, -0.2798, -1.0012,  1.0273,\n",
      "           0.5045, -1.9217,  0.7590, -1.5995,  1.2106,  0.5325,  0.0944,\n",
      "           1.3005,  0.0815]],\n",
      "\n",
      "        [[-0.5735, -0.3089, -1.5566, -0.2495, -0.4468, -0.1620,  0.0901,\n",
      "          -1.9181, -0.0690, -0.3516,  1.3955,  0.7145,  0.1303,  1.6126,\n",
      "          -0.2803,  1.9735],\n",
      "         [-2.3941, -0.2773,  0.1096, -1.6209,  0.5751,  0.2854, -1.2639,\n",
      "           1.0502,  1.0320,  0.4221,  1.1866, -0.2285, -0.4513,  0.9312,\n",
      "          -0.1564,  0.8003],\n",
      "         [-0.5023,  0.3062, -0.4502,  0.5801, -1.8813, -0.6815,  0.8138,\n",
      "           1.1468, -0.2067, -1.5005, -1.1439,  1.5632,  0.1624,  0.1915,\n",
      "           1.7340, -0.1316],\n",
      "         [ 0.6159,  0.1850,  0.6132,  1.3925, -1.5967,  0.7474, -1.1684,\n",
      "           0.5841,  0.8672, -0.6237, -1.3523, -1.3622, -0.9306, -0.2786,\n",
      "           1.0387,  1.2685]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4722, -0.8670,  1.1513,  0.8716,  0.0970, -1.6213, -0.2579,\n",
      "          -0.1375, -0.2392,  1.5019, -0.2585, -0.7533,  0.2551, -0.4288,\n",
      "          -1.2025,  2.3610],\n",
      "         [-0.5671, -0.2779,  1.0956,  0.7926, -0.0216, -1.7280, -0.3405,\n",
      "          -0.2478, -0.3264,  1.4731,  0.3825, -0.8803,  0.1753, -0.5175,\n",
      "          -1.3260,  2.3138],\n",
      "         [-0.6100, -0.9697,  1.0977,  0.7665, -0.0370, -1.8207, -0.4089,\n",
      "          -0.2735, -0.3790,  1.4734,  0.3942, -0.8990,  0.2085, -0.5179,\n",
      "          -0.3899,  2.3653],\n",
      "         [-0.5310, -0.9096,  1.1117,  0.7959, -0.1567, -1.7070, -0.3315,\n",
      "          -0.2183, -0.3069,  1.4576,  0.4141, -0.3098,  0.1744, -0.5214,\n",
      "          -1.2970,  2.3354]],\n",
      "\n",
      "        [[ 0.2701, -0.0796, -1.7033, -0.1933, -0.4943,  1.4349,  1.2847,\n",
      "          -0.5690,  1.6044,  1.0447, -0.6129, -0.2018, -0.3157,  0.5989,\n",
      "          -0.0275, -2.0404],\n",
      "         [ 0.8289,  1.2454, -1.0755, -0.1345, -0.0269,  0.9752,  1.4214,\n",
      "          -0.9358, -0.0281,  1.3392, -0.3158, -0.8745, -0.1270,  0.9310,\n",
      "          -1.5669, -1.6560],\n",
      "         [ 1.2662,  0.6404, -2.6068, -0.8344,  0.3389, -0.6715,  0.3452,\n",
      "          -0.5733,  1.4737,  1.4257, -0.1313,  0.4272,  0.2385,  0.0744,\n",
      "          -0.6152, -0.7975],\n",
      "         [ 1.0814, -1.1976, -0.0798,  0.1950, -0.8443, -0.3946,  0.2606,\n",
      "           0.6372,  0.1305,  1.5187, -0.0713,  1.1635,  0.1249, -1.9378,\n",
      "          -1.7738,  1.1875]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50265])\n",
      "tensor([[[-0.0202,  0.0158, -0.0945,  ...,  0.0804,  0.1224, -0.1314],\n",
      "         [ 0.0142,  0.0303, -0.1050,  ...,  0.0612,  0.0224, -0.1053],\n",
      "         [-0.0238,  0.0365, -0.1340,  ...,  0.0475,  0.0519, -0.0880],\n",
      "         [-0.0837,  0.0265, -0.0657,  ...,  0.1468,  0.1203, -0.1235]],\n",
      "\n",
      "        [[-0.0453, -0.0415,  0.0021,  ..., -0.0533,  0.1263, -0.0591],\n",
      "         [-0.0576, -0.0333, -0.0234,  ...,  0.0065,  0.1910, -0.0556],\n",
      "         [-0.0805,  0.0257,  0.0817,  ...,  0.0409,  0.0625, -0.0747],\n",
      "         [-0.1220, -0.0052,  0.0834,  ...,  0.1382,  0.0995, -0.0588]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 5))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)\n",
    "logits = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
