{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-0.0744,  0.1096,  0.3569, -0.2427, -0.5184, -0.2361,  0.1305,\n",
      "          -0.0049, -0.1922,  0.3523, -0.1591,  0.0055,  0.3097,  0.0006,\n",
      "           0.1725,  0.1720],\n",
      "         [-0.0716,  0.1093,  0.3470, -0.2503, -0.5655, -0.2798,  0.0930,\n",
      "           0.0232, -0.1914,  0.3430, -0.1610,  0.0432,  0.3209, -0.0043,\n",
      "           0.1427,  0.2461],\n",
      "         [-0.1000,  0.1051,  0.3487, -0.2298, -0.5710, -0.2846,  0.1228,\n",
      "           0.0045, -0.2116,  0.3467, -0.1725,  0.0270,  0.3110,  0.0017,\n",
      "           0.1653,  0.2170],\n",
      "         [-0.0990,  0.1039,  0.3398, -0.2383, -0.5440, -0.2337,  0.0908,\n",
      "           0.0086, -0.1968,  0.3508, -0.1634,  0.0414,  0.3007,  0.0112,\n",
      "           0.1338,  0.2188]],\n",
      "\n",
      "        [[ 0.3108, -0.0422,  0.2644, -0.3930,  0.2264, -0.1212, -0.4148,\n",
      "           0.1483, -0.3362, -0.1036, -0.0262,  0.2476,  0.1763,  0.0108,\n",
      "           0.3939, -0.3109],\n",
      "         [ 0.2937, -0.0556,  0.2618, -0.3986,  0.2553, -0.1046, -0.4136,\n",
      "           0.1352, -0.3747, -0.1325,  0.0221,  0.2004,  0.1406,  0.0306,\n",
      "           0.4513, -0.3508],\n",
      "         [ 0.2986, -0.0866,  0.2388, -0.4110,  0.2011, -0.1561, -0.4005,\n",
      "           0.1234, -0.3869, -0.1187, -0.0112,  0.2578,  0.2246,  0.0382,\n",
      "           0.4192, -0.3241],\n",
      "         [ 0.2870, -0.0312,  0.2617, -0.3545,  0.3019, -0.1200, -0.4401,\n",
      "           0.1610, -0.3405, -0.1387, -0.0221,  0.2640,  0.1162,  0.0097,\n",
      "           0.4087, -0.3481]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 0.3327, -0.8196, -1.1490, -1.4489, -0.2774, -0.3396, -0.7298,\n",
      "          -0.2572, -0.8149,  1.7467,  0.0631,  1.3611,  1.2931,  0.1309,\n",
      "          -0.8364,  1.7453],\n",
      "         [ 0.0794,  0.8044,  0.0132, -0.7686, -0.2295, -1.3868,  0.4922,\n",
      "          -1.5978,  0.9453,  0.9738, -0.4922,  0.8979, -2.0766,  0.0068,\n",
      "           1.3225,  1.0161],\n",
      "         [-0.1598,  1.5161, -1.5436, -0.9204,  0.9891, -0.5670,  0.6629,\n",
      "           0.5229, -0.2489,  1.8504, -0.2684, -0.7882,  0.5846,  0.5575,\n",
      "          -1.8714, -0.3158],\n",
      "         [ 0.2629, -0.4137, -0.1411, -1.0722, -1.3532,  0.8676, -0.6836,\n",
      "           0.6728,  1.7002,  0.8968, -0.6476,  0.4595,  0.9586,  1.3140,\n",
      "          -1.1494, -1.6717]],\n",
      "\n",
      "        [[ 0.3345,  0.3017, -1.5003, -0.8372,  0.6681, -0.1755, -1.2749,\n",
      "           1.1982,  2.1567, -1.4367,  0.9975,  0.6556,  0.5090, -0.4519,\n",
      "          -0.4173, -0.7274],\n",
      "         [ 0.1036,  1.3660,  0.3707,  0.8148, -1.4039,  0.3567, -0.5273,\n",
      "          -0.1215, -2.0322,  0.7722, -0.6780,  1.0629,  0.0966,  0.2067,\n",
      "           1.3044, -1.6917],\n",
      "         [-1.5545,  0.1287, -0.5048, -1.2677, -1.3114,  1.8179,  1.0888,\n",
      "          -0.6643, -0.1149, -0.3211,  1.1414,  0.6403,  0.3098, -1.1991,\n",
      "           1.0660,  0.7446],\n",
      "         [ 0.5595, -0.3320, -0.7436,  0.2195,  1.0510,  1.5621,  0.1719,\n",
      "          -2.1376,  0.8145,  0.6169,  0.2906, -1.8172,  1.2625, -0.3609,\n",
      "          -0.6403, -0.5168]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-2.1775e+00, -6.1314e-02, -4.5473e-01,  5.8933e-01,  3.8494e-01,\n",
      "          -6.3279e-02, -9.8499e-01, -6.0135e-01,  1.2112e+00, -1.6528e-01,\n",
      "           1.0329e+00, -1.4881e+00, -2.0659e-01,  4.1656e-02,  1.6526e+00,\n",
      "           1.2905e+00],\n",
      "         [ 1.4260e+00, -8.2967e-03, -5.1471e-01,  1.8634e+00, -4.3680e-01,\n",
      "          -9.6977e-01, -1.7588e+00,  8.6321e-01, -9.1751e-01,  7.2287e-01,\n",
      "          -1.2078e-01,  9.1992e-01, -1.2167e-01,  3.7421e-01, -1.6623e+00,\n",
      "           3.4094e-01],\n",
      "         [-9.3464e-01,  1.0449e+00,  1.3605e+00, -2.4156e-01, -3.4064e-01,\n",
      "           1.4320e+00, -2.5092e-01, -3.8674e-01, -1.0139e+00,  4.4721e-02,\n",
      "           9.0861e-01,  5.3833e-01,  5.0169e-01, -1.3184e+00,  8.6217e-01,\n",
      "          -2.2062e+00],\n",
      "         [-1.5881e+00, -3.8869e-01,  1.3483e+00,  4.0835e-01,  4.5905e-01,\n",
      "           1.5408e-01,  6.6549e-01, -7.5024e-01,  3.0783e-01,  1.1251e+00,\n",
      "           1.3061e+00,  7.7703e-01, -9.3021e-01, -1.9754e+00,  3.2864e-01,\n",
      "          -1.2475e+00]],\n",
      "\n",
      "        [[-5.4244e-02,  1.3599e+00, -6.9448e-01, -7.2193e-01, -4.5900e-01,\n",
      "          -2.5534e-01, -1.7583e+00,  2.1143e+00, -1.0497e-01, -3.8311e-01,\n",
      "           1.3640e+00, -1.0269e+00,  9.5357e-04, -5.3921e-01, -2.1824e-01,\n",
      "           1.3767e+00],\n",
      "         [ 1.1929e+00,  5.0178e-01, -1.1209e+00,  1.4256e-02, -1.1237e+00,\n",
      "           1.3560e+00,  3.9426e-01, -1.1899e+00,  1.6614e+00,  9.3851e-01,\n",
      "          -3.7529e-02,  6.8974e-01, -1.6911e+00, -4.1959e-02, -1.1069e+00,\n",
      "          -4.3667e-01],\n",
      "         [ 1.2673e+00, -7.4392e-01, -1.0072e+00, -6.0734e-01,  5.3855e-01,\n",
      "          -7.3901e-01,  2.9586e-01,  1.2971e+00,  8.1126e-01, -1.7789e+00,\n",
      "          -9.5388e-01, -1.5708e-02, -1.2200e+00,  3.6718e-01,  1.6903e+00,\n",
      "           7.9856e-01],\n",
      "         [ 8.3010e-01,  9.4841e-01, -4.3083e-01,  1.1715e+00, -3.2838e-01,\n",
      "           1.1446e+00, -8.2107e-01,  5.3020e-01, -4.9201e-01, -1.4083e+00,\n",
      "          -4.3956e-01, -1.9121e+00, -3.5230e-01, -2.1259e-02, -4.4053e-01,\n",
      "           2.0216e+00]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-1.0631, -2.2427,  1.2375,  0.2208, -1.1868,  4.9200,  1.0919,\n",
      "           0.1336,  2.0748,  0.2285,  0.2080,  0.0513,  0.4489, -0.5368,\n",
      "          -0.2832, -0.6980],\n",
      "         [-1.3945,  1.1207, -0.9415, -0.9211, -0.8879,  1.5329, -0.4966,\n",
      "          -0.0385, -1.3054,  0.4623,  0.5357, -1.7724,  1.3427,  0.8357,\n",
      "           2.6756, -1.6300],\n",
      "         [ 1.2986, -0.1557,  1.1247, -1.5941,  0.9674,  0.1499,  1.3442,\n",
      "          -1.9106, -0.1675, -0.4938,  0.8028,  0.4982, -1.4611, -1.2048,\n",
      "           0.5588,  1.9044],\n",
      "         [ 0.5611, -1.1732,  1.1563,  0.1908,  0.3299, -1.2589,  0.8412,\n",
      "          -0.9499, -1.0725,  0.6901, -0.5582,  1.3876, -0.8428, -0.7935,\n",
      "          -0.5209, -1.5504]],\n",
      "\n",
      "        [[-0.3652, -0.4517,  1.9866,  0.4457, -0.8511,  3.0066, -0.8788,\n",
      "           0.3108,  1.8888,  1.3156,  0.7894, -1.3409,  1.6299,  0.3381,\n",
      "          -1.0987, -1.5702],\n",
      "         [-1.0823, -0.6783, -0.4636,  1.7522, -0.9803,  0.1611, -1.2355,\n",
      "          -0.6614,  1.2854, -0.2030,  1.5950, -0.6646,  2.3802,  0.1632,\n",
      "           2.0219, -0.1459],\n",
      "         [-0.5991,  0.4003,  0.3577, -0.5826, -3.6568, -1.2478, -1.4709,\n",
      "          -0.3847, -0.3233,  0.4634,  1.7759, -0.7989,  0.7086, -1.8947,\n",
      "          -1.1617, -0.1820],\n",
      "         [ 2.0968, -1.5818, -2.0316,  0.3369,  0.1300, -1.8894, -1.9941,\n",
      "          -2.3928, -1.2132,  0.7688,  0.2311,  3.2127, -3.1131, -0.9110,\n",
      "           0.8561, -1.1568]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 5., 6., 7.],\n",
       "        [2., 4., 1., 0.],\n",
       "        [6., 8., 4., 8.],\n",
       "        [6., 6., 0., 4.],\n",
       "        [3., 5., 9., 5.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1, 4])\n",
      "tensor([[[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0602,  0.1185, -0.5197,  0.3883, -0.2769,  0.5868,  0.8872,\n",
      "           0.1394,  0.3414,  1.2967,  0.8905, -1.3005,  1.6039, -1.4406,\n",
      "          -2.0345,  0.3797],\n",
      "         [-1.0937,  0.5849, -0.4723,  0.6667,  0.3754,  0.4399, -0.4351,\n",
      "           0.3751, -0.0838, -0.2268,  2.0441, -0.7016,  1.5955, -0.7968,\n",
      "          -2.2847,  0.0131],\n",
      "         [-0.4712, -0.3401, -0.4874,  0.3512, -0.0859,  1.1293,  0.7766,\n",
      "           0.8044,  1.0132,  0.4956, -0.1094, -0.7210,  1.9305, -0.6250,\n",
      "          -2.1012, -1.5594],\n",
      "         [ 0.7973, -2.0460, -0.4883, -0.7531, -0.0153,  0.9395,  0.9778,\n",
      "           0.4363,  0.2023,  0.0991,  0.7699,  0.9465,  1.3266, -0.0418,\n",
      "          -1.3764, -1.7742]],\n",
      "\n",
      "        [[-2.3264, -0.1708,  0.0179, -0.5929,  1.4490,  0.1736, -0.0320,\n",
      "           1.2920,  1.1968,  0.3141,  0.7489,  0.0444,  0.5436,  0.1337,\n",
      "          -1.1700, -1.6218],\n",
      "         [-0.6744, -0.0647, -0.4591,  0.0188,  0.0266,  1.9228,  1.7739,\n",
      "          -0.8868,  0.8052,  0.3948,  1.1258, -0.6039,  0.4273, -1.4650,\n",
      "          -1.4813, -0.8600],\n",
      "         [-1.5935, -0.6326,  0.6167, -0.6045,  0.7630,  0.9902, -0.4580,\n",
      "           1.1520,  1.0703,  0.4435,  0.3150,  0.7113,  1.2302, -1.0257,\n",
      "          -1.8554, -1.1225],\n",
      "         [-1.0587,  0.0216, -0.0548,  1.5337,  0.8517,  0.9816, -0.2546,\n",
      "          -1.0510,  1.1381,  0.0947,  1.1709,  0.4006,  0.0394, -1.2096,\n",
      "          -2.2956, -0.3081]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    causal_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = causal_mask(\n",
    "    tgt_len=4,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    tgt_len=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "      (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1131, -0.1536,  0.1407,  ...,  0.0852,  0.0952,  0.1190],\n",
      "         [-0.0821, -0.0387, -0.0959,  ...,  0.0225,  0.0214, -0.0211],\n",
      "         [-0.0823, -0.0334, -0.0518,  ..., -0.0282, -0.0603, -0.0517],\n",
      "         [-0.3557, -0.0599,  0.0321,  ...,  0.0739,  0.0292, -0.0663]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4122,  1.2157, -0.1767,  0.5970,  0.1617, -2.4125, -0.2758,\n",
      "          -0.9491, -0.6338, -0.5471,  1.0070,  0.1406, -1.0475,  1.5750,\n",
      "          -0.0071, -0.0598],\n",
      "         [ 1.1111, -0.8287, -2.0083,  0.9725, -0.0111,  0.6718,  1.3403,\n",
      "          -1.6163, -0.0923,  0.5758, -0.5428, -0.4998,  0.8494,  1.3499,\n",
      "          -0.6033, -0.6683],\n",
      "         [ 0.3647, -1.3126,  1.4006, -0.2067, -0.5217, -0.2639,  0.7043,\n",
      "           0.8158,  0.6458, -1.2123,  0.9422, -2.1219,  1.5767,  0.0918,\n",
      "           0.0267, -0.9296],\n",
      "         [-0.6509,  0.8416, -0.6647,  2.4102, -0.9511, -0.4870,  0.9381,\n",
      "           0.1887, -2.1130,  0.6922,  0.6684,  0.3566, -0.2612, -0.7950,\n",
      "          -0.4904,  0.3176]],\n",
      "\n",
      "        [[ 1.3371,  1.1760, -0.0417,  0.8256,  0.1379, -1.9897, -1.2733,\n",
      "          -0.4875, -0.4235, -0.4504,  1.0214,  0.2570, -0.7912,  1.6655,\n",
      "          -1.1721,  0.2087],\n",
      "         [ 0.1200, -0.6762, -0.2913, -0.0723,  0.2435,  0.1079,  0.2211,\n",
      "           0.0204, -1.6548,  1.3576, -1.7482, -0.2253,  2.2231,  1.2434,\n",
      "          -1.0526,  0.1835],\n",
      "         [ 0.8966,  2.1532, -1.2513, -0.5296,  0.7522, -2.0034, -0.6387,\n",
      "           0.1737, -0.2996, -0.2179,  0.1679,  1.0782, -0.2460,  1.0886,\n",
      "          -1.0573, -0.0666],\n",
      "         [ 0.1939, -0.7573,  0.2595,  0.5005, -1.2319, -1.0337,  1.5871,\n",
      "           0.6301, -1.3964,  0.1819,  2.1507,  0.0396,  0.0634,  0.3830,\n",
      "          -1.6726,  0.1023]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0374e+00, -1.8223e-01, -5.1885e-01, -1.0337e+00,  6.2146e-01,\n",
      "          -5.7532e-01,  1.3011e+00, -1.9218e-01,  2.8882e-01, -7.9383e-01,\n",
      "          -1.2689e+00, -1.2259e+00,  1.5824e-01,  1.3418e+00,  2.1043e+00,\n",
      "           1.0126e+00],\n",
      "         [-3.8584e-01, -1.8447e+00, -1.4931e-01,  9.6935e-02,  1.5092e+00,\n",
      "           1.8017e-01, -4.9214e-01, -5.7917e-02, -3.3987e-02,  2.3276e-01,\n",
      "          -8.2695e-04, -1.5681e-01, -1.8046e+00,  2.4684e+00,  6.2848e-01,\n",
      "          -1.8976e-01],\n",
      "         [-1.1581e+00,  1.5587e+00,  2.2751e-01, -9.4714e-01, -1.2161e-01,\n",
      "          -1.9868e+00,  1.7790e+00, -2.5580e-01, -3.0466e-01,  1.2111e-01,\n",
      "           2.1806e-01,  3.3012e-02,  6.5429e-01, -5.7290e-01,  1.5798e+00,\n",
      "          -8.2447e-01],\n",
      "         [-4.3802e-01, -2.8482e+00,  1.9894e-02, -2.4251e-02, -7.1148e-01,\n",
      "           5.8337e-01,  6.8500e-01,  3.3771e-01, -1.2723e+00,  2.3079e-02,\n",
      "          -2.4077e-01, -5.3852e-02,  5.9618e-01,  8.1610e-01,  1.6936e+00,\n",
      "           8.3392e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan],\n",
      "         [        nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan],\n",
      "         [        nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan],\n",
      "         [        nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan,         nan,         nan,         nan,         nan,\n",
      "                  nan]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BartConfig {\n",
       "   \"activation_dropout\": 0.0,\n",
       "   \"activation_function\": \"gelu\",\n",
       "   \"attention_dropout\": 0.0,\n",
       "   \"bos_token_id\": 0,\n",
       "   \"classifier_dropout\": 0.0,\n",
       "   \"d_model\": 16,\n",
       "   \"decoder_attention_heads\": 16,\n",
       "   \"decoder_ffn_dim\": 4096,\n",
       "   \"decoder_layerdrop\": 0.1,\n",
       "   \"decoder_layers\": 12,\n",
       "   \"decoder_start_token_id\": 2,\n",
       "   \"dropout\": 0.1,\n",
       "   \"encoder_attention_heads\": 16,\n",
       "   \"encoder_ffn_dim\": 4096,\n",
       "   \"encoder_layerdrop\": 0.1,\n",
       "   \"encoder_layers\": 12,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"forced_eos_token_id\": 2,\n",
       "   \"id2label\": {\n",
       "     \"0\": \"LABEL_0\",\n",
       "     \"1\": \"LABEL_1\",\n",
       "     \"2\": \"LABEL_2\"\n",
       "   },\n",
       "   \"init_std\": 0.02,\n",
       "   \"is_encoder_decoder\": true,\n",
       "   \"label2id\": {\n",
       "     \"LABEL_0\": 0,\n",
       "     \"LABEL_1\": 1,\n",
       "     \"LABEL_2\": 2\n",
       "   },\n",
       "   \"max_position_embeddings\": 1024,\n",
       "   \"model_type\": \"bart\",\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"pad_token_id\": 2,\n",
       "   \"scale_embedding\": false,\n",
       "   \"src_vocab_size\": 50265,\n",
       "   \"tgt_vocab_size\": 50265,\n",
       "   \"transformers_version\": \"4.42.0.dev0\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50265\n",
       " },\n",
       " BartSeq2seq(\n",
       "   (inputs_embeds): BartEmbeds(\n",
       "     (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "     (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "   )\n",
       "   (decoder_inputs_embeds): BartEmbeds(\n",
       "     (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "     (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "   )\n",
       "   (encoder): BartEncoder(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (layers): ModuleList(\n",
       "       (0-11): 12 x BartEncoderLayer(\n",
       "         (self_attn): BartAttention(\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "         )\n",
       "         (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (activation_fn): GELU(approximate='none')\n",
       "         (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "         (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "         (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "         (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "     (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (decoder): BartDecoder(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (layers): ModuleList(\n",
       "       (0-11): 12 x BartDecoderLayer(\n",
       "         (self_attn): BartAttention(\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "         )\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (activation_fn): GELU(approximate='none')\n",
       "         (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "         (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "         (encoder_attn): BartAttention(\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "           (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "           (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "         )\n",
       "         (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "         (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "         (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "         (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "     (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "config, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50265])\n",
      "tensor([[[ 0.0234,  0.0852,  0.0187,  ...,  0.0997, -0.0197, -0.0242],\n",
      "         [ 0.0324, -0.0408, -0.0875,  ..., -0.0095,  0.0822,  0.0806],\n",
      "         [-0.0584,  0.0132,  0.0794,  ..., -0.0436, -0.0464, -0.0446],\n",
      "         [ 0.0496,  0.0683, -0.0699,  ...,  0.0742,  0.0520, -0.0190]],\n",
      "\n",
      "        [[-0.0053, -0.0426,  0.0097,  ...,  0.0474,  0.0243,  0.0004],\n",
      "         [ 0.0125,  0.0528,  0.0499,  ...,  0.0844, -0.0011,  0.0659],\n",
      "         [-0.0527,  0.0634,  0.0419,  ...,  0.0414, -0.1528,  0.0520],\n",
      "         [ 0.0286,  0.0942,  0.0424,  ...,  0.1626, -0.0453,  0.0037]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 5))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)\n",
    "logits = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
