{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-0.3085, -0.5430, -0.6939, -0.1806, -0.4738, -0.0900,  0.2146,\n",
      "          -0.0142, -0.0684, -0.4724,  0.1755, -0.4288, -0.2211, -0.6935,\n",
      "          -0.3615,  0.1440],\n",
      "         [-0.2094, -0.4102, -0.9866,  0.8181, -0.2247, -0.9481, -0.0128,\n",
      "           0.0640, -0.1594, -0.4074,  0.1402, -0.1735,  0.0300, -0.6610,\n",
      "           0.0479,  0.9284],\n",
      "         [-0.0788,  0.4413,  0.0049,  0.2629, -0.1417, -0.2024,  0.2738,\n",
      "          -0.0829,  0.4959,  0.5231,  0.0777,  0.0198,  0.3571,  0.2038,\n",
      "           0.3759, -0.4529],\n",
      "         [ 0.1036,  0.6275,  0.1735,  0.4267, -0.1173, -0.1116,  0.5675,\n",
      "           0.1069,  0.5706,  0.2846,  0.2635,  0.3055,  0.5501,  0.3205,\n",
      "           0.3919, -0.4072]],\n",
      "\n",
      "        [[-0.2822, -0.2059, -0.0102,  0.0912, -0.2135,  0.0383,  0.2335,\n",
      "          -0.0493,  0.3750, -0.2267,  0.2058, -0.3390,  0.3391, -0.1193,\n",
      "          -0.1178,  0.1349],\n",
      "         [-0.1455, -0.0861,  0.0957, -0.0336,  0.1165, -0.1926,  0.0992,\n",
      "          -0.1550,  0.5523,  0.0029,  0.0769, -0.1904,  0.3790,  0.0731,\n",
      "           0.1900, -0.1825],\n",
      "         [-0.1519, -0.2582, -0.4743,  0.2055, -0.2108, -0.2621,  0.5101,\n",
      "           0.1496,  0.0628, -0.3109,  0.0691, -0.2203,  0.0457, -0.2554,\n",
      "          -0.1645,  0.2077],\n",
      "         [-0.0598, -0.2793,  0.0436,  0.1727,  0.2356, -0.4213, -0.3361,\n",
      "          -0.2682,  0.4876,  0.4332, -0.1022, -0.2444,  0.4741,  0.2093,\n",
      "           0.2051, -0.1059]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 1.8823,  1.3107, -0.0557, -1.7424, -0.8350,  0.3171,  0.8862,\n",
      "          -1.2024,  0.7124, -0.4691, -0.3853,  0.8529,  0.3980, -0.2549,\n",
      "          -1.6674,  0.2526],\n",
      "         [ 0.3965, -1.2588,  0.0179,  0.5952,  0.0459, -1.7641, -0.6036,\n",
      "          -0.9895,  0.9923,  0.8357,  0.4992,  1.2294,  2.0379, -0.2590,\n",
      "          -0.5660, -1.2088],\n",
      "         [ 0.1841,  0.9893,  0.2465,  1.4967,  0.4635,  0.9032, -0.0817,\n",
      "          -0.9999,  0.2276,  0.9337, -1.5777,  0.0234,  1.0972, -1.6527,\n",
      "          -1.7441, -0.5092],\n",
      "         [ 1.7859,  0.0934, -0.1034, -0.5085,  0.9714, -2.1045,  0.0876,\n",
      "          -1.2743, -0.3106,  1.3316, -0.2501,  0.2765,  1.0097,  0.8613,\n",
      "          -0.8395, -1.0265]],\n",
      "\n",
      "        [[-0.6679,  0.7581,  1.8585,  0.1912, -2.2260,  1.0209, -1.3114,\n",
      "          -0.4396,  0.5835,  0.5375,  0.9441, -1.2929, -0.4306, -0.0978,\n",
      "           0.1709,  0.4016],\n",
      "         [-1.5074, -1.2582,  1.7520, -0.1151,  0.9497,  0.9900,  0.4616,\n",
      "           0.2500,  0.1036,  0.0543,  0.1432, -0.7224, -1.6441,  1.6567,\n",
      "          -0.1826, -0.9311],\n",
      "         [ 0.8428, -0.9140, -0.8618, -1.5439,  0.5010,  0.3179, -0.0352,\n",
      "           0.9998, -0.7207,  2.5931, -0.4205,  0.8557, -0.0900,  0.1438,\n",
      "          -1.2826, -0.3853],\n",
      "         [ 0.2590, -0.3675, -2.6197,  0.7826,  1.3530, -0.5048,  0.0657,\n",
      "          -0.1893,  1.4519, -0.2303,  0.0129, -0.3629,  1.2812,  0.0712,\n",
      "           0.4353, -1.4382]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 1.0776e+00,  1.9157e+00,  8.7856e-02,  1.0362e+00, -5.9893e-02,\n",
      "          -1.1544e+00,  2.8920e-01,  1.6172e+00, -6.4346e-01, -9.8993e-01,\n",
      "          -3.1476e-01, -1.8419e+00, -3.5570e-01, -5.1983e-01, -6.8802e-01,\n",
      "           5.4418e-01],\n",
      "         [-4.1839e-01, -1.4509e+00, -1.1784e+00,  8.5728e-01,  1.0162e+00,\n",
      "          -3.8367e-01, -8.2666e-02, -2.9402e-01,  2.6752e+00, -1.2239e-01,\n",
      "          -5.1743e-01,  7.8535e-02, -4.8309e-01,  1.4130e+00, -6.8115e-01,\n",
      "          -4.2815e-01],\n",
      "         [ 7.7805e-01, -1.6013e+00,  2.1215e-01,  1.7587e+00, -1.7302e+00,\n",
      "          -1.4361e+00,  1.0619e-01, -3.2242e-01, -9.2831e-01,  1.4654e+00,\n",
      "           1.1811e+00,  2.6443e-01,  1.1809e-02,  1.8314e-01, -4.7514e-02,\n",
      "           1.0486e-01],\n",
      "         [ 3.3409e-01,  8.5172e-01, -2.2491e+00,  2.5950e-01, -9.4095e-01,\n",
      "           1.5534e+00,  6.1394e-01,  1.7437e+00, -2.3938e-01,  2.3809e-01,\n",
      "           1.3564e-02, -9.1631e-01, -5.5335e-01, -9.6544e-01,  8.6865e-01,\n",
      "          -6.1215e-01]],\n",
      "\n",
      "        [[ 1.1767e+00, -8.0905e-01,  4.7892e-01, -2.0737e-02, -8.0368e-01,\n",
      "           1.0411e+00,  7.9507e-01, -1.0492e+00,  1.8621e+00,  1.0264e+00,\n",
      "          -1.3606e-01, -2.1939e+00, -2.6650e-02, -2.3294e-03, -5.7167e-01,\n",
      "          -7.6700e-01],\n",
      "         [-3.0927e-01, -7.8285e-01,  8.5517e-01, -1.2423e-01,  3.7514e-01,\n",
      "          -5.7860e-01,  1.4010e+00,  6.9518e-01, -1.2552e+00,  1.6921e+00,\n",
      "           5.6102e-02, -7.6905e-01, -1.4355e+00, -1.5993e+00,  1.3301e+00,\n",
      "           4.4922e-01],\n",
      "         [-2.9453e+00,  2.3360e-01,  1.5282e-01,  4.1915e-01,  8.3677e-01,\n",
      "           1.5192e-01, -8.4341e-01,  5.5222e-01,  8.0718e-01, -1.7296e+00,\n",
      "          -1.0823e-01,  5.8070e-01,  1.9726e-01,  3.3081e-01,  1.0434e+00,\n",
      "           3.2073e-01],\n",
      "         [ 8.7965e-01, -5.8731e-01,  2.5665e+00,  5.4101e-01, -3.9686e-01,\n",
      "          -6.9184e-01, -7.5556e-01,  1.0583e-01,  1.3571e+00,  7.7302e-01,\n",
      "          -1.5013e+00, -1.2815e+00,  1.1206e-01, -2.9744e-01, -3.4876e-01,\n",
      "          -4.7468e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 8.3337e-01,  1.6446e+00, -4.6474e-01,  2.2453e+00, -1.0304e+00,\n",
      "          -1.7082e+00, -1.4529e-02, -6.9212e-01, -5.4204e-01, -8.7379e-01,\n",
      "           1.9477e+00, -1.5175e-01, -1.1088e+00, -2.1477e+00,  1.5735e+00,\n",
      "           8.9000e-01],\n",
      "         [-2.0867e-01,  1.8616e-01,  6.1666e-01, -1.1448e-03, -5.6174e-01,\n",
      "           2.7533e+00,  1.3271e+00,  4.5364e-01,  2.7271e-01, -7.2667e-01,\n",
      "          -7.3358e-01, -8.6063e-02, -1.6477e+00, -4.9940e-01,  2.0937e+00,\n",
      "           6.1895e-01],\n",
      "         [-4.7224e-02,  5.0741e-01, -1.4657e+00,  7.9383e-01,  2.9942e-01,\n",
      "          -5.1116e-02,  1.2519e+00, -1.1645e+00, -9.2459e-01, -2.7501e-01,\n",
      "          -1.8935e+00, -4.9009e-02, -5.2499e-02,  8.2606e-01,  6.0021e-01,\n",
      "          -9.7641e-01],\n",
      "         [-9.1267e-01, -9.6180e-01, -1.9930e+00, -5.1359e-01, -2.4776e+00,\n",
      "           1.0979e+00, -7.2845e-01,  6.2273e-01, -4.8454e-02,  2.8069e+00,\n",
      "           1.2564e+00,  5.9932e-01,  2.6599e+00, -5.1142e-01, -2.7380e+00,\n",
      "           5.8393e-01]],\n",
      "\n",
      "        [[ 6.9845e-01,  1.8650e+00, -1.9257e+00, -6.8774e-01, -4.4425e-01,\n",
      "          -1.0877e+00,  1.2682e+00,  1.1860e+00,  4.4165e-01, -1.3804e-01,\n",
      "           1.2615e+00, -7.5327e-01, -1.9737e+00, -3.1737e+00,  2.0388e+00,\n",
      "          -5.5582e-01],\n",
      "         [ 1.5580e+00, -4.7744e-02,  6.2544e-01,  3.1392e+00, -6.1554e-01,\n",
      "           1.9463e+00,  3.2178e-01, -1.0195e+00,  7.7322e-02,  2.3416e+00,\n",
      "          -1.3693e+00, -1.1158e+00,  1.9890e-02, -1.6263e+00,  1.3985e+00,\n",
      "           5.8585e-01],\n",
      "         [-9.4055e-01, -5.2476e-01,  5.0985e-01, -8.4721e-01, -5.2201e-01,\n",
      "          -2.3359e+00,  1.0488e+00, -2.0473e+00,  9.0359e-01,  7.6232e-01,\n",
      "          -5.5582e-01, -1.1596e+00,  2.2750e+00,  1.0183e+00, -2.3117e+00,\n",
      "           7.1338e-01],\n",
      "         [ 3.1023e-01, -2.1235e-02, -6.5975e-01, -6.1807e-01,  8.9305e-02,\n",
      "           1.8660e-01, -1.1480e+00, -1.3255e+00,  1.8481e+00,  3.5845e+00,\n",
      "           1.7100e+00,  1.0142e+00,  1.0926e+00, -3.8586e-01, -1.8783e+00,\n",
      "           9.3898e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 8., 3., 8.],\n",
       "        [8., 5., 2., 0.],\n",
       "        [3., 1., 2., 4.],\n",
       "        [1., 2., 7., 1.],\n",
       "        [6., 5., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 0, 1],\n",
       "        [1, 1, 0, 1],\n",
       "        [1, 0, 1, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_ids.dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4, 4])\n",
      "tensor([[[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 1],\n",
      "          [1, 1, 0, 1],\n",
      "          [1, 1, 0, 1],\n",
      "          [1, 1, 0, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 1],\n",
      "          [1, 1, 0, 1],\n",
      "          [1, 1, 0, 1],\n",
      "          [1, 1, 0, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 1, 1],\n",
      "          [1, 0, 1, 1],\n",
      "          [1, 0, 1, 1],\n",
      "          [1, 0, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7984, -0.6295, -0.8316,  0.2448,  1.5115, -0.4280, -0.0505,\n",
      "           1.9785, -1.5735,  0.1623,  1.3742, -0.6036,  0.5917, -1.1544,\n",
      "          -1.1564, -0.2341],\n",
      "         [ 0.1040, -1.3184, -1.2163,  0.8586,  2.6831, -0.0876, -0.7086,\n",
      "           0.7585, -0.5279,  0.1150,  0.9749,  0.1732, -0.3672, -0.6455,\n",
      "          -1.2951,  0.4993],\n",
      "         [ 2.1029,  0.0817, -1.3088,  0.4243,  0.2330, -0.4199, -0.4676,\n",
      "           1.7182, -1.2604,  1.3735,  0.4386, -1.4030, -0.1909, -0.3381,\n",
      "          -0.4367, -0.5467],\n",
      "         [-0.0115,  2.0181,  0.5870, -1.5288, -0.4797,  1.1119, -0.3453,\n",
      "           1.6658, -2.0910, -0.5533, -0.2161, -0.0724,  0.1770, -0.2071,\n",
      "           0.1707, -0.2254]],\n",
      "\n",
      "        [[ 1.9773,  0.9269, -0.6369,  0.9566, -1.2002, -0.5251, -0.9857,\n",
      "           0.8696,  0.6430,  0.8418,  0.1626,  0.7989, -0.3029, -1.6982,\n",
      "          -0.7844, -1.0435],\n",
      "         [ 1.7116,  0.7006, -2.2380,  0.8656, -0.8407, -0.4993, -0.8973,\n",
      "           0.9355,  0.7509,  0.8364,  0.4021,  0.7856, -0.1144, -1.1300,\n",
      "          -0.4789, -0.7897],\n",
      "         [ 1.6761,  0.9747, -2.1513,  0.8300, -0.5547, -0.3649, -0.8399,\n",
      "           0.9391,  0.7396,  0.8443,  0.3587,  0.7427, -0.4387, -1.3095,\n",
      "          -0.6897, -0.7566],\n",
      "         [ 1.7696,  0.8000, -2.3424,  0.9213, -0.7733, -0.2607, -0.7158,\n",
      "           0.9624,  0.8974, -0.3813,  0.4777,  0.9134, -0.0130, -0.9792,\n",
      "          -0.6070, -0.6692]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_embeds.dtype,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    causal_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = causal_mask(\n",
    "    bsz=2,\n",
    "    tgt_len=4,\n",
    "    dtype = torch.float32,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "      (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0882,  0.1041, -0.1131,  ..., -0.0465, -0.0664, -0.0375],\n",
      "         [-0.0319,  0.1512, -0.1085,  ..., -0.0740, -0.0571, -0.0241],\n",
      "         [-0.0784,  0.1167, -0.0513,  ..., -0.1046,  0.0120, -0.0367],\n",
      "         [-0.0685,  0.1236, -0.1014,  ..., -0.0820, -0.0500, -0.0364]],\n",
      "\n",
      "        [[-0.0382, -0.0914,  0.0139,  ..., -0.0231,  0.0754, -0.0182],\n",
      "         [-0.0782, -0.0654,  0.0009,  ..., -0.0583,  0.0677, -0.0011],\n",
      "         [-0.0720, -0.0368,  0.0287,  ..., -0.1310,  0.0749,  0.0321],\n",
      "         [ 0.1014,  0.0658, -0.0269,  ..., -0.1334, -0.0450,  0.0677]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0889,  0.3022, -0.2304,  0.0035, -0.9340,  0.7860,  0.2017,\n",
      "          -1.8708, -0.0928,  1.3179,  0.4970, -1.9724, -0.2888,  2.1530,\n",
      "           0.4180, -0.2011],\n",
      "         [-0.1412,  0.3304, -0.1961, -0.0107, -0.9382,  0.8012,  0.2763,\n",
      "          -1.7729, -0.0702,  1.2343,  0.5167, -0.2134, -0.2489,  2.1057,\n",
      "           0.4587, -2.1315],\n",
      "         [-0.1058,  0.3033, -0.2292,  0.0047, -0.9328,  0.7870,  0.2028,\n",
      "          -1.8695, -0.0917,  1.3189,  0.4981, -1.9711, -0.2876,  2.1539,\n",
      "           0.4191, -0.1999],\n",
      "         [-0.1453,  0.4187, -0.0963,  0.0711, -0.7163,  0.8393,  0.3403,\n",
      "          -1.5153,  0.0156,  1.2387,  0.5868, -1.6297, -0.1219,  2.0311,\n",
      "           0.4853, -1.8024]],\n",
      "\n",
      "        [[ 1.3378,  0.2528,  0.0525, -0.2562, -0.3210,  1.0218,  1.2728,\n",
      "           1.7333, -1.3819, -0.3312, -0.0900, -1.3513,  0.6623, -1.2347,\n",
      "           0.1983, -1.5655],\n",
      "         [-2.3883, -0.8351, -0.0574,  1.2195, -0.2198, -0.1900,  0.2054,\n",
      "          -0.7032,  0.4059, -0.9408,  0.9676, -0.7705, -0.0528,  1.0788,\n",
      "           0.3853,  1.8953],\n",
      "         [ 0.0824, -0.9398,  0.3012,  0.0331, -0.8628,  1.4492,  2.2441,\n",
      "           1.8087, -0.9594, -0.5972,  0.1222, -1.0568, -0.1719, -0.8786,\n",
      "          -0.7825,  0.2082],\n",
      "         [ 1.3772, -1.7705,  0.4556, -0.2806,  1.1552, -0.6431, -0.0598,\n",
      "           1.1261, -0.8926, -0.9894,  1.0423, -0.1163, -0.0934,  1.6352,\n",
      "          -1.2980, -0.6476]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0022e-01, -8.6054e-01,  3.3992e-01,  2.6815e+00,  6.2730e-01,\n",
      "           1.6669e-01,  1.2130e-01, -3.2881e-02,  1.0448e+00, -2.7783e-01,\n",
      "          -1.6505e+00, -2.7987e-01,  9.2149e-02, -6.6349e-01, -1.7337e+00,\n",
      "           2.2482e-01],\n",
      "         [ 2.3701e-01, -9.5690e-01,  4.9753e-02,  2.8518e+00,  6.4250e-01,\n",
      "           2.1753e-01, -7.7377e-01, -1.0553e-01,  1.0566e+00, -4.1040e-01,\n",
      "           4.1685e-02, -3.7488e-01,  8.0390e-02, -7.8409e-01, -1.9359e+00,\n",
      "           1.6420e-01],\n",
      "         [ 1.0865e-01, -1.1009e+00,  3.0601e-01,  2.8885e+00,  5.9399e-01,\n",
      "           1.4728e-01, -2.4351e-02, -2.1223e-01,  9.8791e-01, -4.1522e-01,\n",
      "          -1.9474e+00, -4.2343e-01, -9.4312e-02, -8.8358e-01, -2.4735e-02,\n",
      "           9.3811e-02],\n",
      "         [ 1.8808e-01, -8.6628e-01,  3.9576e-01,  2.6639e+00,  6.1464e-01,\n",
      "           3.0690e-01,  1.0748e-01, -9.0711e-02,  1.0249e+00, -2.8934e-01,\n",
      "          -1.6540e+00, -2.9183e-01,  8.3772e-02, -6.7243e-01, -1.7347e+00,\n",
      "           2.1397e-01]],\n",
      "\n",
      "        [[-4.6789e-01,  2.6971e-01, -1.0287e+00,  1.3893e+00, -7.5239e-01,\n",
      "           5.2932e-01, -1.7890e+00, -8.3417e-02,  9.8427e-01,  2.7551e-02,\n",
      "           5.8997e-01,  8.8293e-01, -8.6294e-01, -7.0105e-01,  2.0586e+00,\n",
      "          -1.0463e+00],\n",
      "         [-6.4821e-01, -2.0448e-01, -8.9311e-01,  5.9860e-01, -1.8522e+00,\n",
      "           1.5902e-01, -1.5728e+00, -7.9974e-01,  6.2950e-01,  1.4184e+00,\n",
      "           6.1111e-01,  1.3942e+00, -5.0206e-01, -2.2246e-02,  1.6837e+00,\n",
      "           3.6330e-04],\n",
      "         [ 1.0212e+00, -1.0282e+00, -7.9322e-01, -1.1773e-01, -2.5044e+00,\n",
      "          -4.0425e-02,  4.7416e-01, -5.9206e-01, -8.1007e-01,  8.1243e-01,\n",
      "           1.5441e+00, -3.2243e-01,  3.8786e-01,  6.0859e-01,  1.4408e+00,\n",
      "          -8.0560e-02],\n",
      "         [ 2.1442e-01,  9.1361e-01,  1.2147e+00, -7.0708e-01,  2.1358e-01,\n",
      "          -4.1068e-01,  4.6132e-01, -5.8242e-01, -1.0433e+00, -2.0425e-01,\n",
      "           2.7007e-01,  2.1790e+00, -3.0832e-01,  1.0168e+00, -1.4024e+00,\n",
      "          -1.8251e+00]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50265])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 5))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)\n",
    "logits = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
