{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 0.0389,  0.4104,  0.5358, -0.2459, -0.1781, -0.1642, -0.0216,\n",
      "          -0.1013,  0.1283,  0.0091, -0.2554,  0.5060,  0.0390, -0.1223,\n",
      "           0.1115,  0.0058],\n",
      "         [-0.1039,  0.1530, -0.3803, -0.3977,  0.1306,  0.2841,  0.1005,\n",
      "          -0.2261, -0.0121,  0.1577, -0.1416, -0.0983,  0.0078, -0.2243,\n",
      "           0.2186, -0.3268],\n",
      "         [-0.0913,  0.3246, -1.0315, -0.6313,  0.4344,  0.8401,  0.0937,\n",
      "          -0.6880, -0.1443,  0.1587, -0.5459, -0.1650, -0.3451, -0.4619,\n",
      "          -0.0110, -0.3555],\n",
      "         [-0.1713,  0.1922, -0.0521, -0.3483,  0.0024,  0.1173, -0.0450,\n",
      "          -0.4192, -0.0302,  0.2721, -0.1507,  0.0775, -0.0623, -0.2312,\n",
      "           0.0600, -0.2294]],\n",
      "\n",
      "        [[ 0.3391,  0.5766,  0.0346,  0.1431, -0.2311,  0.2882,  0.3202,\n",
      "           0.0473,  0.3187,  0.4905, -0.3281,  0.3373, -0.1340, -0.2770,\n",
      "           0.2773,  0.0249],\n",
      "         [-0.2525,  0.1176, -0.3275, -0.1843,  0.3619,  0.0377, -0.1492,\n",
      "          -0.2667, -0.1520, -0.2390, -0.3643,  0.4852,  0.0117, -0.0768,\n",
      "           0.4108, -0.3933],\n",
      "         [ 0.0657,  0.3188,  0.1505,  0.2371, -0.1083,  0.0906,  0.0542,\n",
      "           0.0659,  0.1618,  0.4305, -0.1629,  0.3882, -0.0945, -0.0633,\n",
      "           0.3189, -0.2042],\n",
      "         [ 0.0338, -0.4401,  0.6668, -0.3592,  0.0649, -0.1452, -0.1899,\n",
      "           0.0913,  0.1619,  0.0462,  0.2227, -0.2646, -0.1191,  0.4293,\n",
      "           0.2967, -0.3882]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 0.3092, -0.4381, -0.5898,  1.8925, -0.2995,  1.1505, -0.0769,\n",
      "           1.6064,  0.7698, -1.0350, -0.1670, -2.4338, -0.3227, -0.2354,\n",
      "          -0.0586, -0.0715],\n",
      "         [ 1.7055,  0.3381, -1.2048,  1.4162, -1.7015,  0.4368, -1.4465,\n",
      "           0.3767,  0.1977, -0.2445,  0.3203, -0.0442,  0.3787, -1.2158,\n",
      "           1.3776, -0.6904],\n",
      "         [ 0.2369,  2.8296, -1.6335,  1.3096,  0.0176, -0.2577, -0.1917,\n",
      "          -0.5093,  0.6993,  0.3039, -0.7880, -1.1252, -0.1732, -0.2643,\n",
      "           0.2825, -0.7366],\n",
      "         [ 0.9384,  0.2119, -0.5238,  1.7041,  0.9440, -0.9017,  1.0691,\n",
      "          -1.1966, -0.9548, -0.4630, -1.8659,  0.3101, -1.2433,  0.6490,\n",
      "           0.4719,  0.8506]],\n",
      "\n",
      "        [[ 1.5122, -0.9173, -0.9252,  0.8195, -2.3405, -0.2565, -0.7356,\n",
      "           0.2047, -0.0110,  0.4460, -0.5434,  0.6443, -0.8430,  1.1343,\n",
      "           0.3272,  1.4845],\n",
      "         [ 1.0456,  0.3773, -0.8523,  1.4549, -2.6029,  0.2879,  0.3019,\n",
      "          -0.1081, -0.2967,  0.1805,  0.0759,  0.4362,  1.4130, -0.2842,\n",
      "           0.1645, -1.5935],\n",
      "         [-0.4795,  0.0401,  0.6174, -0.5448, -0.5543,  1.0548, -0.9544,\n",
      "           1.3393,  0.3881,  0.7003,  1.5649,  0.6019, -2.3832,  0.0291,\n",
      "          -1.3523, -0.0673],\n",
      "         [ 1.8951, -0.4822, -1.5071,  0.4546,  0.3901,  1.3452, -0.5747,\n",
      "           0.8437, -0.8218, -2.1769, -0.7419,  0.0746,  0.1880,  0.7846,\n",
      "           0.2617,  0.0667]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 0.0838, -1.0741, -0.4239,  1.3732, -0.2004,  0.3073,  2.3867,\n",
      "          -0.6945, -1.3187, -1.3993,  0.6926, -0.5350, -0.6374,  0.1659,\n",
      "           1.2504,  0.0234],\n",
      "         [-1.0046,  1.0142, -0.7813, -1.5916,  2.0711, -0.3687, -0.3814,\n",
      "           0.2923,  0.3803,  0.2298,  1.2389, -0.2420,  0.9088, -1.8127,\n",
      "           0.3830, -0.3360],\n",
      "         [ 0.1838,  1.6697,  0.0334, -1.9656, -0.5262, -0.2398, -1.1432,\n",
      "          -0.3598,  0.1187,  0.8636,  1.6133, -0.4661,  1.1296, -0.5798,\n",
      "           0.9055, -1.2371],\n",
      "         [ 0.7118,  2.5659,  0.7785,  0.0098,  1.2585, -0.4290, -1.2909,\n",
      "          -0.0743, -0.0503, -1.6497, -0.2572, -0.0719, -0.2042,  0.3157,\n",
      "          -0.2467, -1.3660]],\n",
      "\n",
      "        [[ 1.2485,  0.9097, -0.3604,  0.6512, -1.7981, -0.7419,  0.6545,\n",
      "           0.6181, -0.1367,  0.3461, -2.4557,  0.8612, -0.7268,  0.9633,\n",
      "          -0.2379,  0.2050],\n",
      "         [ 0.3879, -0.0709,  0.8146, -1.2012,  0.9592, -0.3175, -0.4683,\n",
      "           0.8780, -1.6584, -2.2175,  0.9524,  0.4321,  1.3582,  0.8902,\n",
      "          -0.5354, -0.2035],\n",
      "         [-1.0104, -0.0265, -0.2640,  1.6131,  0.4574,  0.3027, -0.7824,\n",
      "          -0.9078,  0.6531, -0.1580, -0.8232, -1.7384, -0.1854,  2.4610,\n",
      "           0.5535, -0.1449],\n",
      "         [ 0.3529,  1.0288,  0.0464, -0.3430, -0.8099,  0.4826,  0.9403,\n",
      "           0.9428, -0.4361, -1.3556, -0.0487,  1.0584,  0.5584,  0.8331,\n",
      "          -2.7646, -0.4858]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-0.2306, -2.0944,  2.0371,  0.0823,  1.6332,  1.9889, -1.7074,\n",
      "          -0.6835,  0.3621,  2.1629,  1.7967, -3.8688, -1.2339,  0.3487,\n",
      "          -0.6226, -0.8284],\n",
      "         [-0.9060, -1.5050,  1.6390,  0.1715,  1.1103,  0.8039,  1.6467,\n",
      "          -0.2298, -2.1701,  0.0132, -0.1254, -2.0719,  0.3118, -0.9823,\n",
      "          -3.1595,  0.3988],\n",
      "         [ 1.6639,  0.6652,  0.5490,  1.2204,  0.7428,  0.0290,  0.2879,\n",
      "           0.9791,  0.9325, -0.0320,  0.1458, -2.2266,  0.2778, -0.7276,\n",
      "          -0.5019, -0.5056],\n",
      "         [-0.6863, -0.6860,  0.7250,  1.0308, -1.7031,  1.1196,  1.1003,\n",
      "           1.8988, -3.0791,  0.1530,  0.3555,  0.7907,  0.6875, -1.7792,\n",
      "           1.2669,  0.7915]],\n",
      "\n",
      "        [[ 0.1999, -0.4940,  1.9735,  0.2206,  0.1448,  0.1803, -0.5510,\n",
      "          -1.6708, -0.1974,  0.9242,  1.5357, -3.9122, -0.3895, -0.1896,\n",
      "           0.0173, -0.7235],\n",
      "         [-0.2252, -2.8683,  0.4321,  1.2758,  0.5330, -0.1352,  0.4204,\n",
      "          -3.7060, -1.4523, -1.1843,  1.5800, -2.3727,  1.1623, -4.5857,\n",
      "          -2.5277,  1.4599],\n",
      "         [ 0.5825,  0.5415, -0.1737, -0.9554, -0.9227, -0.4396,  0.6278,\n",
      "          -1.0743, -0.4237, -0.3206,  0.1269, -1.6365, -0.7459,  0.0170,\n",
      "          -1.5968,  0.6234],\n",
      "         [-1.7421, -0.2025, -1.4767,  1.0142, -1.3163,  1.2225,  1.1110,\n",
      "           2.4019, -1.7877,  0.7723,  0.8834,  0.7804,  0.6758, -1.1109,\n",
      "           0.3396,  0.7021]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 5., 4., 7.],\n",
       "        [7., 1., 3., 8.],\n",
       "        [2., 8., 9., 8.],\n",
       "        [1., 2., 8., 7.],\n",
       "        [8., 0., 6., 6.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 0, 1, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_ids.dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4, 4])\n",
      "tensor([[[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1, 1],\n",
      "          [0, 1, 1, 1],\n",
      "          [0, 1, 1, 1],\n",
      "          [0, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 1, 1],\n",
      "          [1, 0, 1, 1],\n",
      "          [1, 0, 1, 1],\n",
      "          [1, 0, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3393e+00, -5.9955e-01,  1.1432e+00,  8.8060e-01,  4.2715e-01,\n",
      "          -1.7897e+00,  5.1543e-01,  5.9620e-03, -3.4387e-01,  1.2885e+00,\n",
      "           9.4253e-01, -1.1165e+00,  7.1777e-01, -8.0670e-01, -1.1938e+00,\n",
      "           1.2683e+00],\n",
      "         [ 1.3529e-01,  8.9397e-01,  7.0699e-01, -1.0973e+00, -1.9791e+00,\n",
      "           2.4163e-01, -1.4070e+00, -1.7463e+00, -4.9425e-02,  5.1930e-01,\n",
      "          -1.8253e-02,  3.5841e-01,  1.3090e-01,  9.5961e-01,  1.6061e+00,\n",
      "           7.4524e-01],\n",
      "         [-1.2204e+00, -1.0538e-01,  2.5693e-01, -1.6847e+00, -8.4828e-01,\n",
      "           1.5640e+00, -5.2388e-02,  1.8177e+00,  1.6162e+00,  6.2449e-01,\n",
      "          -4.6787e-02, -2.8022e-01,  1.6663e-01, -9.7164e-01,  1.8116e-01,\n",
      "          -1.0173e+00],\n",
      "         [-4.7350e-01,  5.0554e-01,  4.5119e-01, -1.3749e+00, -9.6252e-01,\n",
      "           2.0799e+00, -1.4825e-01,  1.1738e+00,  1.9956e+00, -9.8299e-01,\n",
      "          -3.1811e-01, -8.5761e-01,  1.0712e-01, -6.9681e-01,  1.7911e-01,\n",
      "          -6.7759e-01]],\n",
      "\n",
      "        [[ 9.5794e-03, -8.9976e-01,  5.0265e-01,  1.4407e-01,  1.3412e+00,\n",
      "          -1.0425e+00, -6.9905e-02,  1.7281e+00,  1.0212e+00, -2.6727e+00,\n",
      "           3.0153e-01,  3.9656e-01, -6.5980e-01,  2.0968e-01,  4.3029e-02,\n",
      "          -3.5291e-01],\n",
      "         [ 6.2109e-02, -1.5380e+00,  3.1935e-01, -1.4924e-01,  1.5948e+00,\n",
      "          -1.6006e+00, -1.4274e-01,  1.9036e+00,  1.1903e+00,  4.9431e-01,\n",
      "           1.9236e-01,  2.3461e-01, -1.2128e+00,  1.5753e-01, -3.2005e-01,\n",
      "          -1.1856e+00],\n",
      "         [-6.5647e-02,  2.8040e-04,  4.3663e-01,  1.3017e-01,  1.3259e+00,\n",
      "          -1.0262e+00, -2.0582e-01,  1.5602e+00,  9.5472e-01, -2.9217e+00,\n",
      "           1.6918e-01,  6.2069e-01, -7.0648e-01,  1.2897e-01, -5.4625e-02,\n",
      "          -3.4632e-01],\n",
      "         [-1.9068e-02, -8.9753e-01,  4.5149e-01,  2.6449e-01,  1.5224e+00,\n",
      "          -1.0529e+00, -1.0033e-01,  1.4047e+00,  9.8259e-01, -2.6802e+00,\n",
      "           4.1812e-01,  7.6834e-01, -6.7539e-01, -2.1751e-02,  9.0848e-03,\n",
      "          -3.7404e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_embeds.dtype,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    causal_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = causal_mask(\n",
    "    bsz=2,\n",
    "    tgt_len=4,\n",
    "    dtype = torch.float32,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "      (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0083, -0.0343, -0.0034,  ..., -0.0437, -0.0105, -0.0992],\n",
      "         [-0.0062, -0.0340, -0.0076,  ..., -0.0430, -0.0109, -0.1014],\n",
      "         [-0.0083, -0.0342, -0.0034,  ..., -0.0437, -0.0105, -0.0992],\n",
      "         [-0.0005, -0.0337, -0.0013,  ..., -0.0431, -0.0199, -0.0953]],\n",
      "\n",
      "        [[-0.0777,  0.0664,  0.0591,  ..., -0.0485, -0.1908,  0.0668],\n",
      "         [-0.0538,  0.0137,  0.0442,  ..., -0.0513, -0.1541,  0.0685],\n",
      "         [-0.0078,  0.0100, -0.0021,  ..., -0.0747, -0.0642,  0.0212],\n",
      "         [ 0.0102, -0.0777,  0.0335,  ..., -0.0058,  0.0927,  0.0184]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2202,  0.2242,  1.6139,  1.6597, -0.5454,  0.3504, -0.6280,\n",
      "           0.7057,  1.2193, -1.6289, -0.2535, -1.7012, -0.7439, -1.1546,\n",
      "           0.2186,  0.4435],\n",
      "         [-0.3079, -0.3656,  1.6935,  1.7377, -0.4543,  0.4301, -0.5562,\n",
      "           0.7701,  1.2975, -1.5457, -0.1674, -1.6023, -0.6692, -1.0619,\n",
      "           0.2882,  0.5134],\n",
      "         [ 0.3000,  0.4238, -0.2414,  1.9705, -0.4917,  0.5650, -0.5369,\n",
      "           0.9577,  1.4493, -1.6656, -0.1599, -1.7142, -0.6542, -1.1408,\n",
      "           0.3850,  0.5535],\n",
      "         [ 0.1771,  0.2287,  1.6198,  1.6657, -0.5419,  0.3549, -0.6246,\n",
      "           0.7106,  1.2248, -1.6265, -0.2496, -1.6989, -0.7406, -1.1517,\n",
      "           0.2230,  0.4291]],\n",
      "\n",
      "        [[ 1.3516, -1.6218,  1.2227, -1.2537, -1.0342, -0.5422,  1.2619,\n",
      "          -0.6652, -0.0485,  0.5205, -0.0449,  1.4115,  0.7873,  0.4962,\n",
      "          -1.2735, -0.5677],\n",
      "         [-0.8785,  0.5168,  2.2922,  1.0228, -0.0912, -0.6284, -1.2259,\n",
      "          -1.2663, -0.7113,  0.1279,  0.4201,  0.7767,  0.7833,  0.9287,\n",
      "          -0.6891, -1.3778],\n",
      "         [-0.2852,  0.2082, -0.9152, -0.2971, -0.5839, -0.1703, -0.5716,\n",
      "          -0.0894,  1.7870, -1.6989, -0.1734,  0.3696,  1.0932,  0.3897,\n",
      "          -1.2938,  2.2310],\n",
      "         [-0.0981,  1.8422, -0.1235, -1.0285, -0.9089,  1.0449, -1.9053,\n",
      "           1.5174, -0.7593,  0.0264, -0.0959, -0.4570, -0.5082,  1.5980,\n",
      "          -0.2386,  0.0942]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.1328e-01, -5.7728e-01,  2.7410e-01,  4.1479e-01,  5.7958e-01,\n",
      "          -1.7235e+00, -1.0068e+00, -7.8708e-01,  1.4377e+00,  2.5554e+00,\n",
      "          -7.0099e-01,  6.2075e-01, -4.3138e-01, -3.2541e-01,  5.2780e-01,\n",
      "          -2.4447e-01],\n",
      "         [-5.6244e-01, -1.5040e-01,  3.1184e-01,  4.7537e-01,  6.7291e-01,\n",
      "          -1.8050e+00, -3.3451e-01, -7.7196e-01,  1.5899e+00,  2.7304e+00,\n",
      "          -6.8222e-01, -2.3395e-01, -4.5478e-01, -3.0061e-01, -2.7117e-01,\n",
      "          -2.1340e-01],\n",
      "         [-5.6676e-01, -5.3791e-01,  3.3372e-01,  4.8247e-01, -2.5778e-01,\n",
      "          -1.6925e+00, -9.5859e-01, -7.4826e-01,  1.4915e+00,  2.6361e+00,\n",
      "          -6.1420e-01,  7.0141e-01, -3.9079e-01, -2.6217e-01,  5.7732e-01,\n",
      "          -1.9364e-01],\n",
      "         [-4.9494e-01, -4.9082e-01,  3.8996e-01,  5.4073e-01, -1.8499e-01,\n",
      "          -1.6683e+00, -9.3577e-01, -7.0485e-01,  1.5757e+00,  2.7203e+00,\n",
      "          -6.0555e-01, -1.0713e-01, -3.3729e-01, -2.2693e-01,  6.6040e-01,\n",
      "          -1.3055e-01]],\n",
      "\n",
      "        [[-3.5446e-01,  1.7764e-01, -6.1345e-01, -6.1337e-01,  1.1349e+00,\n",
      "          -1.2492e+00, -1.2416e+00,  7.6379e-01, -7.5329e-01, -1.6375e+00,\n",
      "           1.4653e+00,  9.4484e-01,  1.4791e+00,  1.1537e+00, -1.1201e-01,\n",
      "          -5.4443e-01],\n",
      "         [-1.7865e-01, -3.1673e-01,  8.4771e-01, -6.1171e-01,  1.5191e+00,\n",
      "          -9.6508e-01, -1.3766e+00,  1.3558e+00, -1.9490e-01, -1.6671e+00,\n",
      "           7.5310e-01,  3.1247e-01,  1.8532e+00, -6.8923e-01, -3.7408e-04,\n",
      "          -6.4097e-01],\n",
      "         [-1.9550e-01, -1.1142e+00,  9.0550e-01, -7.9980e-01,  1.8827e+00,\n",
      "          -5.0082e-01, -1.3355e+00,  1.2645e+00,  7.1022e-01, -1.9299e+00,\n",
      "           5.6157e-01, -5.2046e-01,  8.2527e-01,  3.1242e-01,  4.6026e-01,\n",
      "          -5.2626e-01],\n",
      "         [ 9.3300e-01, -8.4933e-01, -9.8359e-01,  1.8160e+00,  8.6116e-01,\n",
      "           1.3654e+00,  1.1316e+00,  7.6737e-01, -9.1414e-01, -1.2194e+00,\n",
      "           3.9270e-02, -1.3148e-01, -1.5080e+00, -9.3962e-01, -4.0927e-01,\n",
      "           4.0968e-02]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16, padding_idx=2)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50265])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 5))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)\n",
    "logits = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
