{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-0.0657,  0.3638,  0.3547,  0.1457,  0.2279,  0.1052, -0.2803,\n",
      "          -0.3222,  0.1243,  0.0783,  0.1117,  0.0024, -0.3913, -0.1754,\n",
      "           0.5060,  0.2808],\n",
      "         [-0.2492,  0.1646, -0.0440, -0.1268, -0.0802,  0.0290, -0.1925,\n",
      "          -0.0887, -0.0463,  0.0794,  0.0687, -0.0882, -0.3179, -0.2385,\n",
      "           0.2092,  0.1054],\n",
      "         [-0.3266, -0.0022,  0.1153,  0.1191,  0.2794,  0.0027, -0.0410,\n",
      "           0.0716, -0.1647,  0.1443,  0.2131,  0.1856, -0.3907, -0.1373,\n",
      "           0.5034,  0.0290],\n",
      "         [-0.1162,  0.6666,  0.2486, -0.3639, -0.0015,  0.1141, -0.5059,\n",
      "          -0.5173,  0.4727,  0.2560, -0.0424, -0.2576, -0.0081, -0.1656,\n",
      "           0.3775,  0.4282]],\n",
      "\n",
      "        [[ 0.0750,  0.2250,  0.5008, -0.1996,  0.4376, -0.1817, -0.3130,\n",
      "          -0.4451, -0.1459,  0.1269, -0.3016, -0.4536, -0.3248,  0.3432,\n",
      "          -0.1023,  0.5337],\n",
      "         [-0.0058,  0.3760,  0.3142,  0.2778,  0.2871, -0.2440, -0.3281,\n",
      "          -0.1237,  0.1224,  0.0891,  0.2486, -0.1348, -0.3908, -0.0639,\n",
      "          -0.2902, -0.1043],\n",
      "         [-0.2713,  0.4952, -0.0768, -0.6731, -0.2035,  0.2046,  0.2502,\n",
      "          -0.1296,  0.1162,  0.1890, -0.2958, -0.1306, -0.3078, -0.1488,\n",
      "           0.8689,  0.3328],\n",
      "         [ 0.0832, -0.0686,  0.1738, -0.1472,  0.1992, -0.1989, -0.0884,\n",
      "           0.0054, -0.0663,  0.4885,  0.0418, -0.2630, -0.4230,  0.0532,\n",
      "           0.4106,  0.0932]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 1.7303, -0.0684, -0.5742,  1.0645, -0.1966, -0.0589, -0.4204,\n",
      "           0.5996,  0.3824, -1.5182,  0.3269, -0.2785,  1.4044, -0.1941,\n",
      "          -2.4953,  0.2966],\n",
      "         [ 0.6524, -0.4579, -0.1538,  1.2862,  0.1561, -0.1879,  1.1720,\n",
      "          -0.8738, -0.1441, -0.0564,  0.2808, -0.3219, -1.2976,  2.1412,\n",
      "           0.0438, -2.2390],\n",
      "         [-0.7470,  0.6250,  0.1436,  1.5897, -0.6523,  0.8605, -0.1740,\n",
      "           0.0880, -1.4315, -0.7269, -0.9503,  1.4618,  0.7300, -1.1398,\n",
      "           1.5396, -1.2164],\n",
      "         [ 0.0480, -0.1264, -0.8401, -0.1930,  0.9730,  1.0608,  2.0433,\n",
      "          -0.2330, -1.5247, -1.0239,  0.7541,  1.3340, -1.7605, -0.1561,\n",
      "          -0.0573, -0.2981]],\n",
      "\n",
      "        [[ 0.8921, -1.4672,  0.3682,  1.3168,  1.3081, -0.4817, -0.2093,\n",
      "          -0.3458,  1.4984, -0.9442, -1.3810, -0.8306,  1.1296,  0.4569,\n",
      "          -1.3615,  0.0512],\n",
      "         [ 1.1283,  0.3169,  0.1387,  1.1286, -1.1982,  2.0617, -0.8212,\n",
      "           0.1704, -1.9609, -0.5153,  0.4790, -1.5337,  0.3224,  0.2707,\n",
      "           0.1984, -0.1859],\n",
      "         [ 1.2426,  0.3519,  0.0668,  0.2346,  0.9847,  1.6894, -0.6392,\n",
      "           0.3692, -0.2148,  0.9887,  0.3734, -2.2095, -0.0059, -1.2393,\n",
      "          -0.5853, -1.4073],\n",
      "         [ 0.2185,  1.2549,  0.1867,  0.6934, -1.4899,  0.6181,  0.1122,\n",
      "          -0.3876, -1.1279,  0.7260,  1.4460,  0.2402,  0.6591, -0.5132,\n",
      "          -0.0986, -2.5381]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-1.5776, -0.3299, -1.4813,  0.0822,  0.3337,  1.8795, -0.7751,\n",
      "           2.0257, -0.2111, -1.4173,  0.4328,  0.6545, -0.1010, -0.0264,\n",
      "           0.1103,  0.4010],\n",
      "         [ 0.2131, -1.2151,  0.2691,  1.3050, -0.7477,  0.3791, -2.4871,\n",
      "           0.1070,  1.5471, -0.8608,  0.4454, -0.9378,  0.4249,  1.0222,\n",
      "          -0.0651,  0.6006],\n",
      "         [-0.3252, -1.2854, -0.1554, -1.1722,  0.5934, -0.6179,  0.5589,\n",
      "           0.4534, -0.9157,  1.4623, -0.8523,  2.5113,  0.3034, -0.1923,\n",
      "          -1.0154,  0.6491],\n",
      "         [ 1.5939,  0.4814,  0.6533,  0.8072, -1.2809,  0.4014,  0.0199,\n",
      "           0.4718, -0.0068,  0.0431,  1.0727,  0.2132,  0.0430, -1.8868,\n",
      "          -0.3345, -2.2919]],\n",
      "\n",
      "        [[-0.2395,  1.6573, -2.0908,  0.9395, -2.0119,  0.2139,  0.6822,\n",
      "          -0.5615,  0.1591, -0.3723,  0.7001,  1.3691, -0.3070, -0.5339,\n",
      "           0.3992, -0.0034],\n",
      "         [ 0.9897, -0.4725,  0.6094,  0.2912, -1.4422, -2.1465, -1.1814,\n",
      "           1.2650,  0.7310,  0.6870, -0.3737,  1.1676,  0.9658, -0.9676,\n",
      "           0.3069, -0.4300],\n",
      "         [ 1.1001, -1.5004, -0.5693, -0.7191,  0.1223,  0.0397, -0.7635,\n",
      "           2.1489, -1.1851, -1.2442, -0.1173,  1.2109,  1.0355, -0.0670,\n",
      "           0.9029, -0.3944],\n",
      "         [-0.2746, -1.1166, -0.2133,  0.5248,  0.6987, -1.7756, -1.4067,\n",
      "           1.6235,  0.2410,  0.8235, -0.2655,  0.3871,  0.3798, -1.5687,\n",
      "           0.4002,  1.5426]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 7.2077e-01,  6.3350e-01,  7.1148e-01,  8.9647e-01,  5.6542e-01,\n",
      "          -1.2455e-01,  8.2862e-01,  7.6186e-01, -3.2797e-01,  1.2147e+00,\n",
      "          -2.6743e-01,  2.0352e+00, -1.4536e+00, -1.1206e-01,  6.4773e-01,\n",
      "           4.2575e-01],\n",
      "         [-1.8001e-01, -2.1103e-02,  4.6368e-01, -9.8570e-01, -1.1246e+00,\n",
      "           1.6210e-01, -9.5679e-01,  9.2655e-01, -3.9981e-01,  2.6115e+00,\n",
      "           3.7200e-03, -1.5708e+00,  2.0864e+00, -1.1988e+00,  3.4930e-01,\n",
      "           1.6203e+00],\n",
      "         [ 6.6404e-01, -9.2307e-01,  4.5185e-02,  8.1432e-01, -1.8589e-01,\n",
      "          -1.7871e+00,  9.0726e-01,  1.1230e+00, -1.1461e+00,  7.4479e-01,\n",
      "          -4.6692e-01, -1.8897e-01,  2.9519e+00, -2.6009e+00,  3.2173e+00,\n",
      "           1.7134e+00],\n",
      "         [-2.5679e-01, -2.2951e+00,  7.9971e-01,  5.9659e-01,  2.4703e+00,\n",
      "          -6.5554e-01,  1.4443e+00, -1.2415e+00,  2.0196e-01,  1.0908e+00,\n",
      "           7.0931e-01,  1.2554e+00,  1.3962e-01,  1.1590e-01,  1.6441e+00,\n",
      "           1.9630e+00]],\n",
      "\n",
      "        [[ 8.2142e-01, -4.5370e-01,  1.4970e+00,  4.0508e-01,  1.3879e-01,\n",
      "          -1.6686e+00,  1.1672e+00, -1.1465e+00, -2.0454e+00, -5.8183e-01,\n",
      "           3.9487e-01,  4.1898e+00, -2.0121e+00,  3.0263e+00, -1.0482e+00,\n",
      "          -1.8512e+00],\n",
      "         [-2.2680e+00,  4.4620e-01, -3.2106e-01, -1.4801e+00, -1.3189e+00,\n",
      "          -6.5446e-02,  4.5419e-02, -1.2546e+00, -1.3625e+00,  1.0505e+00,\n",
      "          -8.0502e-01, -1.0612e+00,  1.3605e+00, -5.9381e-01, -1.1356e+00,\n",
      "          -1.2581e+00],\n",
      "         [ 4.3429e-01, -5.0124e-01,  5.6428e-01,  2.7642e-01,  1.0379e+00,\n",
      "          -8.1750e-01,  1.1727e+00,  2.4925e-01, -7.0274e-01,  8.9204e-01,\n",
      "           2.1036e+00, -1.8707e+00,  7.2808e-01, -2.0619e+00,  1.3006e+00,\n",
      "           8.2001e-01],\n",
      "         [-4.5154e-01,  2.3663e+00,  1.3750e-01,  1.8206e+00,  1.4142e+00,\n",
      "           5.1110e-01,  8.8908e-01, -1.1901e+00, -9.9327e-01,  1.9528e-01,\n",
      "           6.0511e-01,  1.8460e+00,  2.6214e-01, -2.2655e+00,  1.0438e+00,\n",
      "           1.0117e+00]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 8., 9.],\n",
       "        [8., 6., 4., 8.],\n",
       "        [4., 4., 8., 5.],\n",
       "        [2., 9., 2., 8.],\n",
       "        [5., 3., 5., 3.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [0, 1, 0, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_ids.dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4, 4])\n",
      "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-3.4028e+38,  0.0000e+00, -3.4028e+38,  0.0000e+00],\n",
      "          [-3.4028e+38,  0.0000e+00, -3.4028e+38,  0.0000e+00],\n",
      "          [-3.4028e+38,  0.0000e+00, -3.4028e+38,  0.0000e+00],\n",
      "          [-3.4028e+38,  0.0000e+00, -3.4028e+38,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2440, -0.7083, -1.2406, -0.4357,  0.6978,  0.2436,  0.4691,\n",
      "           2.5133,  0.8467,  0.8194, -0.9147,  0.6732, -0.1995,  0.2042,\n",
      "          -1.0833, -1.6412],\n",
      "         [ 0.4695,  1.0606, -1.3081,  0.0530,  0.1950, -0.5963,  2.1741,\n",
      "          -1.2432, -1.3733,  0.6151,  0.1743,  0.9148, -1.0729,  0.5024,\n",
      "          -1.1713,  0.6063],\n",
      "         [ 1.8137,  1.6354, -1.7559, -0.3323, -0.1178,  0.0932, -0.8758,\n",
      "           0.3401, -1.4479, -1.1286,  1.5104, -0.3166,  0.1333, -0.1088,\n",
      "           0.2897,  0.2678],\n",
      "         [-1.4007, -0.4752, -1.5265,  0.6131, -0.1875, -0.9110, -0.0320,\n",
      "           0.4401,  0.3301,  1.5506,  0.8130, -0.0623, -0.8966,  1.1299,\n",
      "          -1.2086,  1.8238]],\n",
      "\n",
      "        [[-0.0756,  0.0778,  1.4023,  0.4031, -0.5402,  0.2748, -1.7520,\n",
      "          -0.5551,  0.7526, -0.5615,  2.5652, -1.3054, -0.5864, -0.3779,\n",
      "           0.5988, -0.3205],\n",
      "         [-0.0938,  0.2875,  1.8550,  0.4415, -0.8328,  0.3505, -2.1957,\n",
      "          -0.9362,  1.2270,  0.1041,  0.3740, -1.4050, -0.5145,  1.1419,\n",
      "           0.6181, -0.4215],\n",
      "         [ 0.0550,  0.1235,  1.3196,  0.2645, -0.7211,  0.3003, -1.7828,\n",
      "          -0.6991,  0.7534, -0.5921,  2.4578, -1.1800, -0.8831,  0.5495,\n",
      "           0.4964, -0.4619],\n",
      "         [-0.1036,  0.1913, -0.0542,  0.0793, -0.6534,  0.4120, -1.5926,\n",
      "          -0.4253,  0.9039, -0.6238,  2.7685, -1.2187, -0.8512,  1.1677,\n",
      "           0.3048, -0.3046]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_embeds.dtype,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "      (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0459, -0.0574,  0.0508,  ..., -0.0269, -0.1099, -0.1114],\n",
      "         [-0.0359, -0.1029,  0.0813,  ..., -0.0072, -0.0943, -0.0748],\n",
      "         [-0.0658, -0.0943,  0.0642,  ..., -0.0582, -0.0908, -0.1211],\n",
      "         [-0.0529, -0.0689,  0.0693,  ...,  0.0240, -0.1061, -0.0819]],\n",
      "\n",
      "        [[-0.0456,  0.0967,  0.0799,  ...,  0.1436, -0.0422, -0.0257],\n",
      "         [-0.1097,  0.1285,  0.0124,  ...,  0.1208, -0.0540, -0.0003],\n",
      "         [-0.0469,  0.0368,  0.0976,  ...,  0.2448, -0.0598,  0.1432],\n",
      "         [-0.0197,  0.0679, -0.0665,  ..., -0.0286, -0.1861, -0.0136]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8635,  2.3041, -1.7932,  1.3518, -0.0954, -0.2098, -0.1939,\n",
      "          -0.0780, -0.4081, -0.5802, -0.1218,  1.4588, -0.9903, -0.5278,\n",
      "           0.9309, -0.1837],\n",
      "         [-0.8725,  1.8666, -1.6995,  1.1233, -0.2218, -0.3176, -0.2599,\n",
      "          -0.2234, -0.4828, -0.6526, -0.6089,  1.1081, -1.0264, -0.2576,\n",
      "           0.6417,  1.8834],\n",
      "         [-0.8675,  1.8536, -1.7041,  1.1128, -0.2318, -0.1019,  0.1153,\n",
      "          -0.6798, -0.5081, -0.6687, -0.5868,  1.1001, -0.9942, -0.3109,\n",
      "           0.6320,  1.8399],\n",
      "         [-0.9228,  1.8465, -1.7593,  1.1136, -0.2642, -0.1361,  0.1395,\n",
      "          -0.7179, -0.5594, -0.7325, -0.6225,  1.1011, -0.3124, -0.6430,\n",
      "           0.6140,  1.8555]],\n",
      "\n",
      "        [[-0.8596,  0.8030,  1.0868, -0.1257,  1.9796, -0.4293,  1.0498,\n",
      "          -0.8805, -0.4387,  0.1243,  0.8115,  0.3964, -1.6538, -0.8856,\n",
      "          -1.5885,  0.6102],\n",
      "         [-0.8797,  0.9185,  0.9745, -1.6029,  1.5181,  1.7618, -0.4743,\n",
      "           1.3820,  0.0327, -0.7501, -0.9968,  0.3501, -0.8202, -0.5807,\n",
      "          -0.0406, -0.7926],\n",
      "         [ 0.3916,  0.2912, -0.1561, -2.1689,  1.8465,  2.0680, -0.1553,\n",
      "          -0.6766,  0.5768, -0.7765, -0.0760,  0.5375, -0.8303, -0.9490,\n",
      "          -0.1061,  0.1832],\n",
      "         [-0.1622, -1.0078, -0.0978,  0.0779, -1.9724,  0.3338,  0.8077,\n",
      "          -0.2635, -1.2051,  0.2477,  1.8174,  0.7722,  0.9222, -1.7086,\n",
      "           0.7422,  0.6964]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8187,  0.2092,  0.9143, -0.7915, -0.7106,  0.4006,  1.7924,\n",
      "           0.1419, -0.1151, -1.1451, -0.4593,  0.9742,  0.0084, -0.5595,\n",
      "          -1.8067,  1.9655],\n",
      "         [-0.1004,  0.1759,  0.8809, -0.8474, -0.7765,  0.3759,  1.7623,\n",
      "           0.0716, -0.1628, -1.2141, -0.5415,  0.9799, -0.0414, -0.6297,\n",
      "          -1.8970,  1.9644],\n",
      "         [-0.8146,  0.2435,  0.9425, -0.6830, -0.6681,  0.4360,  1.8092,\n",
      "           0.1730, -0.6987, -1.0835, -0.4259,  0.9992,  0.0393, -0.5137,\n",
      "          -1.7278,  1.9723],\n",
      "         [-0.8668,  0.1463,  0.9320, -0.7835,  0.0392,  0.3955,  1.7319,\n",
      "           0.1592, -0.7400, -1.1482, -0.4745,  0.9998,  0.0175, -0.5757,\n",
      "          -1.8039,  1.9711]],\n",
      "\n",
      "        [[ 0.2280,  0.3130, -0.1226,  0.3363, -0.8759, -0.3423, -1.0096,\n",
      "           1.5788,  0.0254,  0.2069, -0.4179,  0.3443,  2.0786, -0.8594,\n",
      "          -2.2957,  0.8122],\n",
      "         [ 0.2113,  1.5571, -0.0256, -1.8141, -0.5693,  0.7893, -0.9420,\n",
      "           0.9509, -0.0355,  0.4615, -0.5491,  0.3327,  1.7134, -0.4833,\n",
      "          -1.9006,  0.3032],\n",
      "         [ 1.2625,  1.2931,  1.1743, -2.0239, -0.6690,  0.5847,  0.4673,\n",
      "           0.5088,  0.5743, -1.0074, -1.9985,  0.0317,  0.4135,  0.2063,\n",
      "          -0.6376, -0.1803],\n",
      "         [-1.1743,  1.5594,  1.7867, -0.9792, -0.6282,  0.8918,  0.2649,\n",
      "          -1.3624,  0.2548,  0.2108,  0.2229, -1.9150,  0.7245,  0.3052,\n",
      "          -0.5627,  0.4007]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50265])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 5))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)\n",
    "logits = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
