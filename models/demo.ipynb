{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamngocthi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bart_model_from_scratch.multihead_attn import BartAttention\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "config.pad_token_id = 2\n",
    "config.encoder_layerdrop = 0.1\n",
    "config.decoder_layerdrop = 0.1\n",
    "config.d_model = config.encoder_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartAttention(\n",
      "  (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bart_attn = BartAttention(\n",
    "    embed_dim=config.d_model,\n",
    "    num_heads=config.encoder_attention_heads,\n",
    "    dropout=config.attention_dropout,\n",
    ")\n",
    "print(bart_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 1.0462,  0.3592,  0.2246, -0.5626,  0.1817,  0.5669,  0.6548,\n",
      "           0.5279, -0.2823, -0.3640,  1.2936, -1.0978,  0.2974, -0.6205,\n",
      "          -1.0247, -0.0346],\n",
      "         [-0.3718, -0.1760,  0.3206,  0.0215,  0.2763, -0.4573,  0.0340,\n",
      "          -0.7747,  0.6343, -0.1275,  0.3659,  0.6905, -0.4680,  0.4454,\n",
      "           0.1758, -0.4128],\n",
      "         [-0.3091,  0.1989,  0.6613,  0.4054, -0.0936, -0.2836, -0.5086,\n",
      "          -0.6263,  0.9526, -0.1183, -0.0350,  0.6673, -0.5660,  0.4035,\n",
      "           0.4103, -0.0595],\n",
      "         [-0.5635,  0.0474,  0.2046,  0.2334,  0.5169, -0.8230,  0.2165,\n",
      "          -0.5140,  0.4842, -0.0744,  0.0973,  0.8233, -0.4679,  0.7096,\n",
      "           0.4349, -0.2063]],\n",
      "\n",
      "        [[-0.2984,  0.3075, -0.0122,  0.1216, -0.0853,  0.2248,  0.1031,\n",
      "          -0.1167, -0.2619,  0.1801, -0.0073,  0.1607, -0.2764,  0.2743,\n",
      "           0.2203,  0.0849],\n",
      "         [-0.0372,  0.3480, -0.1320, -0.0014,  0.1742,  0.2192,  0.4463,\n",
      "           0.3129, -0.2551, -0.1676,  0.3612, -0.1570, -0.0689,  0.2291,\n",
      "          -0.2865, -0.1338],\n",
      "         [ 0.6605, -0.4863, -0.1319, -0.5524,  0.4126,  0.3103,  1.0423,\n",
      "           0.6880, -0.4159, -0.0042,  0.5020, -0.6135,  0.9352,  0.0995,\n",
      "          -1.0896, -0.5678],\n",
      "         [-0.4232,  0.6206,  0.0365,  0.3259,  0.2659, -0.0850, -0.0200,\n",
      "          -0.2776,  0.2278,  0.1570,  0.1321,  0.6061, -0.6583,  0.1962,\n",
      "           0.2754,  0.1483]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_attn\n",
    "hidden_states = torch.randn(2, 4, config.d_model)\n",
    "output = bart_attn(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder_layer import BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_encoder_layer = BartEncoderLayer(config)\n",
    "bart_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 1.8030e+00, -2.3779e-01,  7.2607e-01, -2.1844e+00, -1.8298e-01,\n",
      "           2.4605e-01, -1.0806e+00,  1.0848e+00, -2.6457e-01, -1.2902e+00,\n",
      "           8.7900e-02, -1.0991e-01,  2.8431e-01, -3.1176e-01,  1.7152e+00,\n",
      "          -2.8511e-01],\n",
      "         [-1.2394e+00, -1.7937e+00,  1.6529e+00,  8.2487e-02, -4.2537e-01,\n",
      "           5.5730e-01, -1.0607e+00, -2.3867e-02,  7.4978e-01,  1.2225e+00,\n",
      "           4.7909e-01, -5.3035e-01,  1.5226e+00,  2.1020e-01, -1.4000e+00,\n",
      "          -3.4838e-03],\n",
      "         [-1.9641e-01, -1.0728e+00, -9.2217e-01,  1.0100e+00, -6.7216e-01,\n",
      "           7.4513e-01,  2.7732e-01, -1.2440e+00, -1.0530e+00,  1.7775e+00,\n",
      "           1.8320e+00, -1.4973e+00,  6.0952e-01,  7.7392e-02,  2.3515e-01,\n",
      "           9.3876e-02],\n",
      "         [-1.1667e+00,  5.8067e-01, -7.1737e-01,  2.8107e-01,  8.7349e-01,\n",
      "           1.1116e-01,  2.6952e-01,  9.9211e-01, -1.1643e-01,  9.3360e-01,\n",
      "           1.3948e+00,  2.4243e-02, -9.6769e-01, -8.3995e-01,  9.1114e-01,\n",
      "          -2.5636e+00]],\n",
      "\n",
      "        [[ 1.5372e-03, -4.2497e-01, -2.8888e-01,  4.3412e-01, -1.6303e+00,\n",
      "          -2.8072e-01,  4.5776e-01,  2.6082e+00, -8.5135e-03,  1.0527e+00,\n",
      "           1.3378e-02, -8.1873e-01, -4.6582e-01, -1.5797e+00,  1.1265e+00,\n",
      "          -1.9658e-01],\n",
      "         [-1.0730e-01, -1.5445e+00, -1.5689e+00,  7.1105e-01,  3.8295e-01,\n",
      "          -2.4458e-01,  9.3364e-01, -1.4217e+00,  1.5760e+00,  1.7357e+00,\n",
      "           1.7905e-01,  8.2195e-01,  1.8605e-01, -9.3621e-01, -6.4627e-01,\n",
      "          -5.6938e-02],\n",
      "         [ 6.6911e-01, -8.6655e-01, -1.4108e+00,  1.0707e+00, -1.0378e-01,\n",
      "           2.3302e-01, -6.6349e-01, -3.5450e-01,  2.1806e-01,  2.1185e+00,\n",
      "          -1.2556e+00,  1.4507e+00,  5.5045e-01,  2.6519e-01, -1.5074e+00,\n",
      "          -4.1377e-01],\n",
      "         [ 7.9347e-01,  1.9391e+00, -1.3807e+00,  1.3754e+00, -4.8645e-01,\n",
      "          -1.9367e-02, -1.8880e+00,  3.2554e-01, -9.9649e-01,  3.3760e-01,\n",
      "          -5.9835e-01,  1.4882e-01, -2.1143e-01,  8.4150e-01, -9.9815e-01,\n",
      "           8.1743e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "output = bart_encoder_layer(hidden_states)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder_layer import BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation_fn): GELU(approximate='none')\n",
       "  (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder_layer = BartDecoderLayer(config)\n",
    "bart_decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-1.5444e+00, -1.8321e+00, -4.1857e-01,  1.5340e+00, -8.1430e-01,\n",
      "           1.8727e-03,  2.0293e+00, -3.5316e-02,  3.0787e-02,  8.0391e-02,\n",
      "           1.0235e+00, -7.0789e-01,  3.7463e-01,  2.2874e-01, -7.4908e-01,\n",
      "           7.9839e-01],\n",
      "         [ 9.7598e-01,  1.2249e+00, -1.4192e+00,  4.6237e-01,  1.3616e+00,\n",
      "           2.7617e-01, -1.3110e+00,  3.1597e-01, -2.3389e+00, -1.5550e-01,\n",
      "          -2.8599e-01,  9.2186e-01, -5.4749e-01, -5.3114e-01,  4.5421e-01,\n",
      "           5.9622e-01],\n",
      "         [ 2.5801e-01, -1.3298e+00, -1.9878e-01, -2.0574e-01, -1.1998e+00,\n",
      "           1.6933e+00,  5.9514e-01,  8.1902e-01,  1.1743e+00, -9.5891e-01,\n",
      "           2.0939e-01, -4.1938e-01,  4.4432e-01, -2.1860e+00,  2.0231e-01,\n",
      "           1.1027e+00],\n",
      "         [ 1.3444e+00, -3.1018e-01, -7.0288e-01,  7.6817e-02,  1.5185e+00,\n",
      "          -7.6392e-01, -1.6996e+00,  2.5525e-01, -4.3272e-01,  6.4973e-01,\n",
      "           6.1103e-01,  1.5664e+00, -1.3402e+00,  6.9883e-01, -4.6585e-02,\n",
      "          -1.4248e+00]],\n",
      "\n",
      "        [[ 9.9117e-01,  3.7817e-01,  4.5342e-01, -2.5730e+00,  2.5071e-01,\n",
      "           1.5589e+00, -3.3651e-02,  3.0754e-01,  5.8778e-01, -2.2014e-01,\n",
      "          -6.5795e-01,  1.4701e+00, -1.4833e-01, -1.3723e+00, -3.0393e-01,\n",
      "          -6.8849e-01],\n",
      "         [-7.2791e-01,  1.4552e-01,  2.8965e-01, -8.2963e-01,  2.7654e+00,\n",
      "           3.4419e-01,  9.8643e-01, -4.4014e-01, -3.1308e-01,  5.6107e-01,\n",
      "          -1.1046e+00, -8.3717e-01, -1.3802e+00, -7.0753e-01,  2.8829e-01,\n",
      "           9.5974e-01],\n",
      "         [-8.2767e-02,  1.0056e+00,  3.4430e-01,  2.6923e+00, -5.6422e-01,\n",
      "          -1.1026e+00, -5.5540e-01, -1.0106e+00,  4.7780e-01, -1.1070e-01,\n",
      "           5.7966e-01, -1.4195e+00,  6.5593e-01,  6.3180e-01, -5.0450e-01,\n",
      "          -1.0371e+00],\n",
      "         [ 1.3545e+00, -1.0667e+00,  2.6572e-01,  4.4284e-01,  1.0692e-01,\n",
      "          -4.0880e-01, -8.0360e-01,  3.1948e-01,  1.9594e+00,  4.7861e-01,\n",
      "          -1.9056e-01, -1.4438e+00, -1.6146e+00,  1.5907e+00, -5.8932e-01,\n",
      "          -4.0073e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder_layer\n",
    "hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model, dtype=torch.float32)\n",
    "print(hidden_states.shape)\n",
    "print(encoder_hidden_states.shape)\n",
    "output = bart_decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    ")\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.embeds import BartEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = 50265\n",
    "config.tgt_vocab_size = 50265\n",
    "bart_embeds = BartEmbeds(\n",
    "    num_embeddings=config.src_vocab_size,\n",
    "    embedding_dim=config.d_model,\n",
    "    padding_idx=config.pad_token_id,\n",
    "    max_position_embeddings=config.max_position_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n",
      "tensor([[[-2.4160e+00, -1.9661e+00,  2.6443e+00, -9.3280e-01,  2.4465e+00,\n",
      "           2.4268e-01,  1.7166e+00,  2.3802e+00,  1.3417e+00, -5.4644e-01,\n",
      "          -1.1550e+00, -1.1335e+00,  1.3520e+00,  1.0609e+00,  7.9171e-01,\n",
      "           9.2121e-01],\n",
      "         [-4.5567e-01, -4.8341e-01,  8.6834e-03,  3.6270e+00,  1.8820e+00,\n",
      "          -1.5477e+00, -7.3921e-01,  7.3150e-01, -5.2626e-01,  1.2476e+00,\n",
      "          -3.4702e-01,  1.3448e+00,  4.2299e-01, -1.9006e+00, -2.0581e-01,\n",
      "           1.8824e-01],\n",
      "         [ 3.1356e-01, -4.2852e-01,  3.4789e-01,  1.5724e+00,  6.7705e-01,\n",
      "           5.0431e-01, -3.0831e-01, -5.6241e-01, -1.8508e-01,  2.2599e+00,\n",
      "           2.6357e+00,  8.5528e-01,  5.7900e-01, -1.1962e+00,  1.2859e+00,\n",
      "           1.8313e+00],\n",
      "         [-9.6787e-01, -1.2522e+00, -5.0352e-01, -6.1392e-01, -4.1005e+00,\n",
      "           7.1405e-01,  8.3517e-01,  6.4711e-01,  2.5311e+00, -2.9036e+00,\n",
      "           2.1044e+00,  2.6219e-01, -1.7285e+00, -1.4205e-02, -1.1527e+00,\n",
      "          -4.2345e-01]],\n",
      "\n",
      "        [[-9.2321e-01, -6.5322e-01,  1.9457e+00, -1.8699e-01, -4.7794e-01,\n",
      "          -1.2763e+00,  3.1131e+00,  2.7215e+00, -5.8910e-01,  6.5009e-01,\n",
      "          -6.1098e-01,  1.1298e+00,  1.6729e+00, -1.3726e+00,  1.0607e+00,\n",
      "          -6.6796e-01],\n",
      "         [-2.6833e-01, -2.6615e+00,  1.9492e+00,  1.0187e+00,  1.7282e+00,\n",
      "           4.3909e-01, -2.8349e+00, -1.9731e-01, -1.0542e+00,  9.7708e-01,\n",
      "           4.0706e-01, -2.0563e-01, -6.0093e-01,  1.3692e+00,  2.0577e-02,\n",
      "           1.1998e+00],\n",
      "         [-4.8543e-01,  3.5172e-01, -1.1913e+00, -1.1138e-01, -1.0402e-02,\n",
      "          -8.3893e-02,  1.2041e+00, -1.2354e+00, -4.8048e-01, -1.5062e+00,\n",
      "           1.2832e+00,  7.4711e-01,  3.3226e-01, -1.7142e+00,  9.8692e-01,\n",
      "           1.4992e+00],\n",
      "         [-2.6893e+00, -9.3458e-01,  1.2221e+00, -2.2488e+00, -1.7732e+00,\n",
      "          -1.1676e+00, -9.3308e-01, -2.1906e-02,  1.6483e+00, -2.5683e+00,\n",
      "           6.8734e-01,  1.7419e+00,  4.7415e-04, -5.0786e-01, -2.8749e-01,\n",
      "           4.8661e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test BartEmbeds\n",
    "input_ids = torch.randint(0, config.src_vocab_size, (2, 4))\n",
    "output = bart_embeds(input_ids)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_encoder_atn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 3., 2.],\n",
       "        [5., 1., 3., 3.],\n",
       "        [2., 5., 3., 8.],\n",
       "        [8., 4., 2., 0.],\n",
       "        [4., 7., 9., 7.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_encoder_mask\n",
    "input_ids = torch.randint(0, 10, (5, 4)).to(torch.float32)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 1, 0, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_ids.dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4, 4])\n",
      "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-3.4028e+38,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [-3.4028e+38,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [-3.4028e+38,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [-3.4028e+38,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00, -3.4028e+38,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_attention_mask.shape)\n",
    "print(encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.encoder import BartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = BartEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5404, -1.9294, -1.0220, -0.7080, -1.1214,  1.3517, -0.0978,\n",
      "           0.0620, -0.4251,  1.3274,  0.2395,  0.3164, -0.1805,  1.0341,\n",
      "          -1.1895,  0.8022],\n",
      "         [ 0.6196, -1.1127, -0.8184,  0.9903, -0.4060,  1.0541, -1.4214,\n",
      "          -0.8607, -0.8077, -0.2565,  1.2298, -0.6836, -0.3138,  1.1268,\n",
      "          -0.4438,  2.1040],\n",
      "         [ 0.5989, -0.5086, -0.5109,  0.9121, -0.6072, -0.0942, -0.6524,\n",
      "           0.8949, -0.2833,  0.5651,  1.8712, -0.1361, -0.2460,  0.7344,\n",
      "          -2.8415,  0.3035],\n",
      "         [ 0.8179, -0.6423, -0.4396,  0.6302,  0.7321, -0.3719, -0.9057,\n",
      "           0.1630, -0.5038, -0.7877,  1.9304,  0.5566,  0.1752,  1.0278,\n",
      "          -2.6003,  0.2183]],\n",
      "\n",
      "        [[-0.9234,  0.6368, -0.7670,  0.1170, -2.0568,  0.8087, -0.3666,\n",
      "           0.8740, -1.3334,  0.3223, -0.1272, -0.3249,  1.2975,  1.1477,\n",
      "          -0.8994,  1.5949],\n",
      "         [-0.9555,  0.7023, -0.8008,  0.1991, -2.1627,  1.2012, -0.3068,\n",
      "           0.8193, -0.3725, -0.9814,  0.0968, -0.3057,  1.1400,  1.1082,\n",
      "          -0.9365,  1.5550],\n",
      "         [-0.8334,  0.9323, -0.6279,  0.2649, -2.0221,  1.0077, -0.2369,\n",
      "           1.0555, -1.1447, -0.8803,  0.1634,  0.0765,  1.3350,  0.0679,\n",
      "          -0.8900,  1.7321],\n",
      "         [-0.8291,  0.9123, -0.6492,  0.3094, -1.9172, -0.3976, -0.1968,\n",
      "           0.8708, -1.1934, -0.8326,  0.5086, -0.1719,  1.2881,  1.2725,\n",
      "          -0.7364,  1.7624]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test bart_encoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "# attention_mask = torch.randint(0, 2, (2, 4))\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 0],\n",
    "    ]\n",
    ")\n",
    "encoder_mask = create_encoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=input_embeds.dtype,\n",
    ")\n",
    "# print(f\"{encoder_mask=}\")\n",
    "# print(f\"{input_embeds.shape=}, {attention_mask.shape=}\")\n",
    "# print(f\"{input_embeds=}, {attention_mask=}\")\n",
    "output = bart_encoder(input_embeds, attention_mask)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.mask.create_decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.utils.mask import (\n",
    "    create_decoder_atn_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test causal_mask\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "dtype = torch.float32\n",
    "create_decoder_atn_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.decoder import BartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x BartDecoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "      (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_decoder = BartDecoder(config)\n",
    "bart_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# test bart_decoder\n",
    "input_embeds = torch.randn(2, 4, config.d_model)\n",
    "attention_mask = torch.randint(0, 2, (2, 4))\n",
    "encoder_hidden_states = torch.randn(2, 4, config.d_model)\n",
    "encoder_attention_mask = torch.randint(0, 2, (2, 4))\n",
    "output = bart_decoder(\n",
    "    input_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    ")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_model_from_scratch.model_seq2seq import BartSeq2seq\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSeq2seq(\n",
       "  (inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16)\n",
       "  )\n",
       "  (decoder_inputs_embeds): BartEmbeds(\n",
       "    (embed_tokens): Embedding(50265, 16, padding_idx=2)\n",
       "    (embed_positions): Embedding(1024, 16)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=16, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=16, bias=True)\n",
       "        (final_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=16, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartSeq2seq(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "input_ids = torch.randint(0, 10, (2, 4))\n",
    "attention_mask = (input_ids != config.pad_token_id).to(torch.int64)\n",
    "decoder_input_ids = torch.randint(0, 10, (2, 4))\n",
    "decoder_attention_mask = (decoder_input_ids != config.pad_token_id).to(torch.int64)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(decoder_input_ids.shape)\n",
    "print(decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1565,  0.0515,  0.0264,  ..., -0.0720, -0.0294,  0.1270],\n",
      "         [-0.1412,  0.0390,  0.0277,  ..., -0.0550, -0.0214,  0.1114],\n",
      "         [-0.1214,  0.0116,  0.0260,  ..., -0.0756, -0.0678,  0.0531],\n",
      "         [-0.0986, -0.0128,  0.0336,  ..., -0.1044, -0.0647,  0.1063]],\n",
      "\n",
      "        [[ 0.0861, -0.0086, -0.0126,  ...,  0.0174, -0.0422, -0.0960],\n",
      "         [ 0.0518,  0.0299, -0.0117,  ...,  0.0144, -0.0465, -0.0912],\n",
      "         [ 0.0906, -0.0103, -0.0112,  ...,  0.0239, -0.0375, -0.0987],\n",
      "         [ 0.0595, -0.0018, -0.0103,  ..., -0.0074, -0.0586, -0.0788]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=decoder_attention_mask,\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0280e+00,  8.8693e-01,  2.3429e-01,  1.8027e-01, -1.0213e-01,\n",
      "           4.2868e-01, -4.7562e-01,  9.8743e-01,  1.4823e+00, -3.0026e-01,\n",
      "          -4.1284e-01, -8.2776e-01,  1.8947e+00, -3.1237e-01, -2.2919e-01,\n",
      "          -2.4064e+00],\n",
      "         [ 2.8718e-01,  9.6391e-01,  7.0305e-01, -2.7565e-01, -1.4134e-01,\n",
      "           4.8724e-01, -5.8417e-01,  1.1320e+00,  1.7588e+00, -3.6381e-01,\n",
      "          -4.6213e-01, -1.0921e+00,  3.2876e-01,  2.5883e-01, -2.2959e-01,\n",
      "          -2.7710e+00],\n",
      "         [-1.0619e+00,  1.2940e+00,  9.2832e-01, -7.7382e-02,  8.9972e-02,\n",
      "           7.2326e-01, -4.4333e-01,  1.4173e+00,  4.2797e-01, -1.7117e-01,\n",
      "          -3.1551e-01, -9.3816e-01,  5.5609e-01,  4.7057e-01, -9.6243e-02,\n",
      "          -2.8038e+00],\n",
      "         [-9.6978e-01,  8.6515e-01,  5.8152e-01, -2.6557e-01,  1.4826e-01,\n",
      "           4.2492e-01, -5.5264e-01,  9.7091e-01,  1.4558e+00, -3.1750e-01,\n",
      "          -4.0760e-01, -8.4619e-01,  1.8706e+00, -3.4542e-01, -2.2946e-01,\n",
      "          -2.3829e+00]],\n",
      "\n",
      "        [[-1.3795e-01,  8.0308e-02,  1.7169e+00,  4.2229e-01,  2.6595e-01,\n",
      "           7.4738e-01, -1.3163e+00,  8.1933e-01, -1.7878e+00, -1.1754e+00,\n",
      "           2.6556e-01, -1.2930e+00,  2.5996e-02,  6.4021e-01,  1.5508e+00,\n",
      "          -8.2423e-01],\n",
      "         [-7.9374e-01, -7.4941e-01,  2.5944e+00, -1.0112e+00,  1.0047e-01,\n",
      "          -6.5100e-01, -8.6264e-02, -1.5880e+00,  1.1183e+00,  1.5565e-01,\n",
      "           3.4371e-01, -4.8524e-02,  4.2239e-01,  5.9751e-01,  8.1744e-01,\n",
      "          -1.2217e+00],\n",
      "         [-1.6543e-02,  1.1553e-01, -3.7495e-01, -1.8266e-01, -1.7837e+00,\n",
      "           3.3224e-02,  4.3503e-01, -2.2983e+00,  1.5590e+00,  4.6543e-01,\n",
      "           7.0032e-01,  5.1599e-01, -5.1137e-01,  1.3149e+00, -9.2506e-01,\n",
      "           9.5306e-01],\n",
      "         [-1.1041e+00,  8.8212e-01, -8.8757e-02,  2.7033e-01, -1.7177e+00,\n",
      "           7.9370e-01,  1.1900e+00, -1.7620e+00,  9.8073e-01, -1.6633e-02,\n",
      "           8.3752e-01, -2.0584e-01, -1.6740e+00,  7.4746e-01, -1.3253e-03,\n",
      "           8.6837e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = model.get_encoder_out(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "print(encoder_out.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5736, -1.4813,  1.3059,  2.0250, -0.9025, -1.4694, -0.7691,\n",
      "          -0.9636, -0.2674,  1.2345,  0.3042, -0.5916,  0.0626,  0.0927,\n",
      "          -0.1964,  1.0428],\n",
      "         [-0.2215, -0.2653,  1.4288,  2.3012, -1.3321, -0.2015, -1.1367,\n",
      "          -1.3015, -0.4530,  1.2909,  0.2138, -0.8577, -0.0424, -0.0749,\n",
      "          -0.3334,  0.9853],\n",
      "         [ 0.7770, -1.6532,  1.6490,  0.0758, -0.9033, -1.5593, -0.7597,\n",
      "          -0.9696, -0.1642,  1.6285,  0.4898, -0.5231,  0.2482,  0.2717,\n",
      "           0.0343,  1.3580],\n",
      "         [ 0.6811, -1.5119,  1.4260,  2.1407, -0.8595, -1.4520, -0.7183,\n",
      "          -0.9457, -0.2241,  1.3537,  0.3898, -0.5356,  0.1296,  0.1727,\n",
      "          -0.0851,  0.0386]],\n",
      "\n",
      "        [[ 0.2736,  0.1025, -0.1866, -0.0939,  1.6130, -1.3404, -0.5224,\n",
      "          -0.5799, -0.2519, -1.0836,  0.1243,  0.7459,  1.8023, -0.0640,\n",
      "           1.4122, -1.9511],\n",
      "         [ 0.5043,  0.0218, -0.8330, -1.3302,  1.4892, -0.9727, -0.3571,\n",
      "          -0.3810, -0.0162, -0.7692,  0.1619,  0.7481,  1.9161,  0.0217,\n",
      "           1.4753, -1.6790],\n",
      "         [ 0.3862,  0.3107, -0.8861, -1.3362,  1.4355, -1.0696, -0.3905,\n",
      "          -0.4766, -0.1644, -0.0085,  0.1078,  0.6834,  1.8663, -0.0339,\n",
      "           1.4492, -1.8733],\n",
      "         [-0.0741,  0.2176, -0.9218, -1.2357,  1.4995, -0.0971, -0.4372,\n",
      "          -0.4080, -0.1488, -1.0146,  0.1238,  0.6820,  2.0133,  0.1421,\n",
      "           1.4560, -1.7970]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_out = model.get_decoder_out(\n",
    "    input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    encoder_hidden_states=encoder_out.last_hidden_state,\n",
    "    encoder_attention_mask=attention_mask\n",
    ")\n",
    "print(decoder_out.last_hidden_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
